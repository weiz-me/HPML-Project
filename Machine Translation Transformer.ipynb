{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc792ff0-aeaa-4ab8-9a79-c3a86f34c24a",
   "metadata": {},
   "source": [
    "# Part 0 :Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfefbaae-4767-496b-be11-ba86318fae98",
   "metadata": {},
   "source": [
    "### Downloading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55fbfd07-af73-4e0a-8077-a8e70e4b3577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘spa.txt’ already there; not retrieving.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cd data && wget -nc https://lazyprogrammer.me/course_files/nlp3/spa.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474973f5-ef9c-4d79-8a93-6059208a2417",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f7fbe2-f338-4fd1-b0f9-9c46cbca2adb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c31cb871-e9a2-40a3-9267-358a0c6f3edb",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41b96be8-15fc-42a9-8f9d-bbc3695e681e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>es</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Ve.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Vete.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Vaya.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Hola.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Run!</td>\n",
       "      <td>¡Corre!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     en       es\n",
       "0   Go.      Ve.\n",
       "1   Go.    Vete.\n",
       "2   Go.    Vaya.\n",
       "3   Hi.    Hola.\n",
       "4  Run!  ¡Corre!"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data/spa.txt', sep=\"\\t\", header=None)\n",
    "df.columns = ['en', 'es']\n",
    "df = df.iloc[:10_000] # takes too long (smaller sample)\n",
    "df.to_csv('data/spa.csv', index=None)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba244c79-7056-4e32-8a9b-af31ffd5bf60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/wei/.local/lib/python3.10/site-packages (4.46.2)\n",
      "Requirement already satisfied: datasets in /home/wei/.local/lib/python3.10/site-packages (3.1.0)\n",
      "Requirement already satisfied: sentencepiece in /home/wei/.local/lib/python3.10/site-packages (0.2.0)\n",
      "Requirement already satisfied: sacremoses in /home/wei/.local/lib/python3.10/site-packages (0.1.1)\n",
      "Requirement already satisfied: requests in /home/wei/.local/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: filelock in /home/wei/.local/lib/python3.10/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/wei/.local/lib/python3.10/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/wei/.local/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/wei/.local/lib/python3.10/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/wei/.local/lib/python3.10/site-packages (from transformers) (2.1.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/wei/.local/lib/python3.10/site-packages (from transformers) (4.67.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/wei/.local/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/wei/.local/lib/python3.10/site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: pandas in /home/wei/.local/lib/python3.10/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: fsspec[http]<=2024.9.0,>=2023.1.0 in /home/wei/.local/lib/python3.10/site-packages (from datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /home/wei/.local/lib/python3.10/site-packages (from datasets) (3.11.2)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/wei/.local/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/wei/.local/lib/python3.10/site-packages (from datasets) (18.0.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/wei/.local/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: xxhash in /home/wei/.local/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: joblib in /home/wei/.local/lib/python3.10/site-packages (from sacremoses) (1.4.2)\n",
      "Requirement already satisfied: click in /home/wei/.local/lib/python3.10/site-packages (from sacremoses) (8.1.7)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/wei/.local/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/wei/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.17.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/wei/.local/lib/python3.10/site-packages (from aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/wei/.local/lib/python3.10/site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/wei/.local/lib/python3.10/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/wei/.local/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/wei/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/wei/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/wei/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/wei/.local/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/wei/.local/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/wei/.local/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/wei/.local/lib/python3.10/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/wei/.local/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/wei/.local/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/wei/.local/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets sentencepiece sacremoses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d84ca44-2d00-4927-98cc-ec6a3a487968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aa3be4d8b314476b825e25cf84eae76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "raw_dataset = load_dataset('csv', data_files='data/spa.csv')\n",
    "split = raw_dataset['train'].train_test_split(test_size=0.3, seed=42)\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-es\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "414e12ce-7500-48b1-a28f-e6d4df0573f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "es_sentence='Tom dejó entrar a Mary.'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['▁Tom', '▁dejó', '▁entrar', '▁a', '▁Mary', '.', '</s>']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### looking at sample tokenizer\n",
    "en_sentence = split[\"train\"][0][\"en\"]\n",
    "es_sentence = split[\"train\"][0][\"es\"]\n",
    "\n",
    "inputs = tokenizer(en_sentence)\n",
    "targets = tokenizer(text_target=es_sentence)\n",
    "\n",
    "print(f\"{es_sentence=}\")\n",
    "tokenizer.convert_ids_to_tokens(targets['input_ids'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d986c900-3e3f-45c1-ac31-3d69f8924822",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "\n",
    "def preprocess_function(batch):\n",
    "    model_inputs = tokenizer(\n",
    "        batch['en'], max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Set up the tokenizer for targets\n",
    "    labels = tokenizer(\n",
    "        text_target=batch['es'], max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfb658b1-7430-4c32-81d9-cfede329b2e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf3f9c0abcd5427fab2f7d4399c9d346",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "363fad47eff740b28a7d9e4764379405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 7000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = split.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=split[\"train\"].column_names,\n",
    ")\n",
    "tokenized_datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3f5e1b0-5aa2-418d-9aff-549480917165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch['input_ids']=tensor([[ 5266,   965,  4963,    16,     3,     0],\n",
      "        [ 9680,   282,     3,     0, 65000, 65000],\n",
      "        [  124,   864,   167,     3,     0, 65000],\n",
      "        [ 1086,  1030,    55,     0, 65000, 65000],\n",
      "        [   33,   675,  2778,     3,     0, 65000]])\n",
      "batch['attention_mask']=tensor([[1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 0]])\n",
      "batch['labels']=tensor([[ 5266,  7464,  3531,     8,  4963,     3,     0],\n",
      "        [ 1259, 31575, 18368,  2266,     3,     0,  -100],\n",
      "        [  131, 43360,     3,     0,  -100,  -100,  -100],\n",
      "        [  107, 47657,    55,     0,  -100,  -100,  -100],\n",
      "        [ 1686,  7474,     3,     0,  -100,  -100,  -100]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer)\n",
    "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(0, 5)])\n",
    "batch.keys()\n",
    "print(f\"{batch['input_ids']=}\")\n",
    "print(f\"{batch['attention_mask']=}\")\n",
    "print(f\"{batch['labels']=}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b33444b9-7a39-4115-8faf-786164e73da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.all_special_ids=[0, 1, 65000]\n",
      "tokenizer.all_special_tokens=['</s>', '<unk>', '<pad>']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"{tokenizer.all_special_ids=}\")\n",
    "print(f\"{tokenizer.all_special_tokens=}\")\n",
    "tokenizer.add_special_tokens({\"cls_token\": \"<s>\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c3f9114-3ab8-4a49-9365-301a98720790",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=128,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    tokenized_datasets[\"test\"],\n",
    "    batch_size=128,\n",
    "    collate_fn=data_collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92cfe74-4a89-4af8-bf03-ca0feeead6a6",
   "metadata": {},
   "source": [
    "# Model\n",
    "### Loading and training base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24977e4f-1f5d-4e7b-8db8-3f21c0c01d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.Transformer import Transformer,Encoder,Decoder\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import dataset\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4651bd6c-7aec-4c11-8313-5c7eac8a5646",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_size=tokenizer.vocab_size + 1,\n",
    "                  max_len=512,\n",
    "                  d_k=16,\n",
    "                  d_model=64,\n",
    "                  n_heads=4,\n",
    "                  n_layers=2,\n",
    "                  dropout_prob=0.1)\n",
    "decoder = Decoder(vocab_size=tokenizer.vocab_size + 1,\n",
    "                  max_len=512,\n",
    "                  d_k=16,\n",
    "                  d_model=64,\n",
    "                  n_heads=4,\n",
    "                  n_layers=2,\n",
    "                  dropout_prob=0.1)\n",
    "transformer = Transformer(encoder, decoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b808257c-622b-48db-8078-36efdbf25818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (embedding): Embedding(65002, 64)\n",
       "  (pos_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_blocks): Sequential(\n",
       "    (0): DecoderBlock(\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha1): MultiHeadAttention(\n",
       "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (mha2): MultiHeadAttention(\n",
       "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (ann): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): DecoderBlock(\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha1): MultiHeadAttention(\n",
       "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (mha2): MultiHeadAttention(\n",
       "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (ann): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  (fc): Linear(in_features=64, out_features=65002, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "encoder.to(device)\n",
    "decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1f5c3c3-ecf4-4546-ac7c-89af50f87b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "optimizer = torch.optim.Adam(transformer.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62d0af97-74ef-4b87-8877-06442f27f5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# A function to encapsulate the training loop\n",
    "def train(model, criterion, optimizer, train_loader, valid_loader, epochs):\n",
    "    \n",
    "  train_losses = np.zeros(epochs)\n",
    "  test_losses = np.zeros(epochs)\n",
    "  time_dataloading = np.zeros(epochs)\n",
    "  time_training = np.zeros(epochs)\n",
    "  time_epoch = np.zeros(epochs)\n",
    "  train_accuracy = np.zeros(epochs)\n",
    "  test_accuracy = np.zeros(epochs)\n",
    "    \n",
    "    \n",
    "    \n",
    "  for it in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    dataloading_time = 0\n",
    "    epoch_time = 0\n",
    "    training_time = 0\n",
    "      \n",
    "    torch.cuda.synchronize()\n",
    "    epoch_time_start = time.perf_counter() # epoch start\n",
    "    torch.cuda.synchronize()\n",
    "    dataloading_time_start = time.perf_counter() # dataloading\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        torch.cuda.synchronize()\n",
    "        dataloading_time_end = time.perf_counter() # dataloading\n",
    "        \n",
    "        # move data to GPU (enc_input, enc_mask, translation)\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        batch_time_start = time.perf_counter() # batch training start\n",
    "\n",
    "      # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        enc_input = batch['input_ids']\n",
    "        enc_mask = batch['attention_mask']\n",
    "        targets = batch['labels']\n",
    "\n",
    "      # shift targets forwards to get decoder_input\n",
    "        dec_input = targets.clone().detach()\n",
    "        dec_input = torch.roll(dec_input, shifts=1, dims=1)\n",
    "        dec_input[:, 0] = 65_001\n",
    "\n",
    "      # also convert all -100 to pad token id\n",
    "        dec_input = dec_input.masked_fill(\n",
    "            dec_input == -100, tokenizer.pad_token_id)\n",
    "\n",
    "      # make decoder input mask\n",
    "        dec_mask = torch.ones_like(dec_input)\n",
    "        dec_mask = dec_mask.masked_fill(dec_input == tokenizer.pad_token_id, 0)\n",
    "\n",
    "      # Forward pass\n",
    "        outputs = model(enc_input, dec_input, enc_mask, dec_mask)\n",
    "        loss = criterion(outputs.transpose(2, 1), targets)\n",
    "        \n",
    "      # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "        batch_time_end = time.perf_counter() # batch training end\n",
    "        training_time += (batch_time_end-batch_time_start)\n",
    "\n",
    "        train_loss.append(loss.item())\n",
    "        \n",
    "        # Compute accuracy\n",
    "        predictions = torch.argmax(outputs, dim=2)  # Shape: (batch_size, seq_len)\n",
    "        mask = targets != -100  # Ignore padded tokens in accuracy calculation\n",
    "        train_correct += (predictions[mask] == targets[mask]).sum().item()\n",
    "        train_total += mask.sum().item()\n",
    "\n",
    "        dataloading_time += (dataloading_time_end-dataloading_time_start)\n",
    "        torch.cuda.synchronize()\n",
    "        dataloading_time_start = time.perf_counter() # dataloading\n",
    "\n",
    "    # Get train loss and test loss\n",
    "    train_loss = np.mean(train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = []\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "\n",
    "    for batch in valid_loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        enc_input = batch['input_ids']\n",
    "        enc_mask = batch['attention_mask']\n",
    "        targets = batch['labels']\n",
    "\n",
    "      # shift targets forwards to get decoder_input\n",
    "        dec_input = targets.clone().detach()\n",
    "        dec_input = torch.roll(dec_input, shifts=1, dims=1)\n",
    "        dec_input[:, 0] = 65_001\n",
    "\n",
    "      # change -100s to regular padding\n",
    "        dec_input = dec_input.masked_fill(\n",
    "            dec_input == -100, tokenizer.pad_token_id)\n",
    "\n",
    "      # make decoder input mask\n",
    "        dec_mask = torch.ones_like(dec_input)\n",
    "        dec_mask = dec_mask.masked_fill(dec_input == tokenizer.pad_token_id, 0)\n",
    "\n",
    "        outputs = model(enc_input, dec_input, enc_mask, dec_mask)\n",
    "        loss = criterion(outputs.transpose(2, 1), targets)\n",
    "        test_loss.append(loss.item())\n",
    "\n",
    "      # Compute accuracy\n",
    "        predictions = torch.argmax(outputs, dim=2)  # Shape: (batch_size, seq_len)\n",
    "        mask = targets != -100  # Ignore padded tokens in accuracy calculation\n",
    "        test_correct += (predictions[mask] == targets[mask]).sum().item()\n",
    "        test_total += mask.sum().item()\n",
    "\n",
    "    test_loss = np.mean(test_loss)\n",
    "\n",
    "    # Save losses\n",
    "    train_losses[it] = train_loss\n",
    "    test_losses[it] = test_loss\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    epoch_time_end = time.perf_counter() # epoch end\n",
    "    epoch_time += (epoch_time_end-epoch_time_start)\n",
    "    time_epoch[it] = epoch_time\n",
    "    time_dataloading[it]= dataloading_time\n",
    "    time_training[it]= training_time\n",
    "    train_accuracy[it]= train_correct/train_total\n",
    "    test_accuracy[it]=test_correct/test_total\n",
    "\n",
    "    print(f'Epoch {it+1}/{epochs}, Train Loss: {train_loss:.4f}, Training Accuracy: {train_correct/train_total:.4f}\\\n",
    "      Test Loss: {test_loss:.4f}, Test Accuracy: {test_correct/test_total:.4f}, Epoch Time: {epoch_time:.4f}, Dataloading Time: {dataloading_time:.4f}, Training Time: {training_time:.4f}')\n",
    "\n",
    "  return train_losses, test_losses,time_epoch,time_dataloading,time_training,train_accuracy,test_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "483b2862-3507-4161-8d87-eb7306101484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Train Loss: 8.0534, Training Accuracy: 0.2984      Test Loss: 5.5909, Test Accuracy: 0.3461, Epoch Time: 3.7507, Dataloading Time: 0.1980, Training Time: 2.8425\n"
     ]
    }
   ],
   "source": [
    "train_losses, test_losses,time_epoch,time_dataloading,time_training,train_accuracy,test_accuracy = train(\n",
    "    transformer, criterion, optimizer, train_loader, valid_loader, epochs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2917d0e9-6631-406c-97b4-77e889e33250",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b53ae9b-80ab-4189-a6aa-dec89939e473",
   "metadata": {},
   "source": [
    "### Optimize Dataloading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53be4e6-4b3a-4aa6-a452-46d30925e85f",
   "metadata": {},
   "source": [
    "#### num_of_worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e33174a-94ae-43ec-90be-76819bff45b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "nw =1\n",
      "Epoch 1/5, Train Loss: 4.9862, Training Accuracy: 0.3636      Test Loss: 4.7039, Test Accuracy: 0.3991, Epoch Time: 3.2357, Dataloading Time: 0.2048, Training Time: 2.3607\n",
      "Epoch 2/5, Train Loss: 4.3710, Training Accuracy: 0.4123      Test Loss: 4.3497, Test Accuracy: 0.4333, Epoch Time: 3.1923, Dataloading Time: 0.2094, Training Time: 2.2616\n",
      "Epoch 3/5, Train Loss: 4.0449, Training Accuracy: 0.4383      Test Loss: 4.1165, Test Accuracy: 0.4581, Epoch Time: 3.2282, Dataloading Time: 0.1968, Training Time: 2.3634\n",
      "Epoch 4/5, Train Loss: 3.7987, Training Accuracy: 0.4584      Test Loss: 3.9205, Test Accuracy: 0.4756, Epoch Time: 3.0539, Dataloading Time: 0.1896, Training Time: 2.1969\n",
      "Epoch 5/5, Train Loss: 3.5795, Training Accuracy: 0.4784      Test Loss: 3.7627, Test Accuracy: 0.4960, Epoch Time: 3.2352, Dataloading Time: 0.3599, Training Time: 2.2111\n",
      "\n",
      "nw =2\n",
      "Epoch 1/5, Train Loss: 3.3827, Training Accuracy: 0.4954      Test Loss: 3.6432, Test Accuracy: 0.5075, Epoch Time: 3.1592, Dataloading Time: 0.2453, Training Time: 2.2419\n",
      "Epoch 2/5, Train Loss: 3.2107, Training Accuracy: 0.5077      Test Loss: 3.5359, Test Accuracy: 0.5162, Epoch Time: 3.4160, Dataloading Time: 0.3369, Training Time: 2.2807\n",
      "Epoch 3/5, Train Loss: 3.0455, Training Accuracy: 0.5198      Test Loss: 3.4196, Test Accuracy: 0.5275, Epoch Time: 3.1124, Dataloading Time: 0.2054, Training Time: 2.2520\n",
      "Epoch 4/5, Train Loss: 2.8973, Training Accuracy: 0.5317      Test Loss: 3.3360, Test Accuracy: 0.5349, Epoch Time: 3.0477, Dataloading Time: 0.2139, Training Time: 2.2000\n",
      "Epoch 5/5, Train Loss: 2.7523, Training Accuracy: 0.5428      Test Loss: 3.2625, Test Accuracy: 0.5444, Epoch Time: 2.9727, Dataloading Time: 0.2132, Training Time: 2.1232\n",
      "\n",
      "nw =4\n",
      "Epoch 1/5, Train Loss: 2.6239, Training Accuracy: 0.5522      Test Loss: 3.1943, Test Accuracy: 0.5508, Epoch Time: 3.1869, Dataloading Time: 0.2298, Training Time: 2.2916\n",
      "Epoch 2/5, Train Loss: 2.5011, Training Accuracy: 0.5617      Test Loss: 3.1462, Test Accuracy: 0.5565, Epoch Time: 3.3251, Dataloading Time: 0.2254, Training Time: 2.3641\n",
      "Epoch 3/5, Train Loss: 2.3806, Training Accuracy: 0.5711      Test Loss: 3.0938, Test Accuracy: 0.5611, Epoch Time: 3.1353, Dataloading Time: 0.2402, Training Time: 2.2236\n",
      "Epoch 4/5, Train Loss: 2.2688, Training Accuracy: 0.5824      Test Loss: 3.0432, Test Accuracy: 0.5706, Epoch Time: 3.0383, Dataloading Time: 0.2361, Training Time: 2.1696\n",
      "Epoch 5/5, Train Loss: 2.1632, Training Accuracy: 0.5921      Test Loss: 3.0157, Test Accuracy: 0.5745, Epoch Time: 3.1636, Dataloading Time: 0.2301, Training Time: 2.2071\n",
      "\n",
      "nw =8\n",
      "Epoch 1/5, Train Loss: 2.0600, Training Accuracy: 0.6009      Test Loss: 2.9807, Test Accuracy: 0.5810, Epoch Time: 3.5540, Dataloading Time: 0.3572, Training Time: 2.3435\n",
      "Epoch 2/5, Train Loss: 1.9665, Training Accuracy: 0.6140      Test Loss: 2.9513, Test Accuracy: 0.5835, Epoch Time: 3.5940, Dataloading Time: 0.3977, Training Time: 2.4289\n",
      "Epoch 3/5, Train Loss: 1.8683, Training Accuracy: 0.6242      Test Loss: 2.9329, Test Accuracy: 0.5881, Epoch Time: 3.1753, Dataloading Time: 0.3045, Training Time: 2.1984\n",
      "Epoch 4/5, Train Loss: 1.7820, Training Accuracy: 0.6346      Test Loss: 2.9141, Test Accuracy: 0.5962, Epoch Time: 3.5077, Dataloading Time: 0.3433, Training Time: 2.3005\n",
      "Epoch 5/5, Train Loss: 1.7026, Training Accuracy: 0.6445      Test Loss: 2.8933, Test Accuracy: 0.6020, Epoch Time: 3.7478, Dataloading Time: 0.3693, Training Time: 2.3999\n",
      "\n",
      "nw =16\n",
      "Epoch 1/5, Train Loss: 1.6183, Training Accuracy: 0.6573      Test Loss: 2.8795, Test Accuracy: 0.6078, Epoch Time: 4.1418, Dataloading Time: 0.6160, Training Time: 2.7141\n",
      "Epoch 2/5, Train Loss: 1.5400, Training Accuracy: 0.6671      Test Loss: 2.8714, Test Accuracy: 0.6088, Epoch Time: 3.4503, Dataloading Time: 0.4471, Training Time: 2.3384\n",
      "Epoch 3/5, Train Loss: 1.4669, Training Accuracy: 0.6777      Test Loss: 2.8757, Test Accuracy: 0.6119, Epoch Time: 3.2817, Dataloading Time: 0.4507, Training Time: 2.1564\n",
      "Epoch 4/5, Train Loss: 1.4053, Training Accuracy: 0.6881      Test Loss: 2.8730, Test Accuracy: 0.6166, Epoch Time: 3.2837, Dataloading Time: 0.4923, Training Time: 2.1206\n",
      "Epoch 5/5, Train Loss: 1.3380, Training Accuracy: 0.7007      Test Loss: 2.8706, Test Accuracy: 0.6191, Epoch Time: 3.4434, Dataloading Time: 0.4606, Training Time: 2.2792\n"
     ]
    }
   ],
   "source": [
    "number_workers= [1,2,4,8,16]\n",
    "trainloading = []\n",
    "\n",
    "for nw in number_workers:\n",
    "    print(f\"\\n{nw =}\")\n",
    "    train_loader = DataLoader(\n",
    "        tokenized_datasets[\"train\"],\n",
    "        shuffle=True,\n",
    "        batch_size=128,\n",
    "        collate_fn=data_collator,\n",
    "        num_workers = nw\n",
    "    )\n",
    "\n",
    "    train_losses, test_losses,time_epoch,time_dataloading,time_training,train_accuracy,test_accuracy = train(\n",
    "        transformer, criterion, optimizer, train_loader, valid_loader, epochs=5)\n",
    "\n",
    "    trainloading.append(np.mean(time_dataloading))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d596cfec-bf7f-46dd-8ed4-094cc66e1b02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(number_workers, trainloading, marker='o', linestyle='-', color='b', label='Train Loading Time')\n",
    "\n",
    "# Add labels, title, and grid\n",
    "plt.xlabel('Number of Workers', fontsize=12)\n",
    "plt.ylabel('Train Loading Time (seconds)', fontsize=12)\n",
    "plt.title('Train Loading Time vs Number of Workers', fontsize=14)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.legend(fontsize=10)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ee5d8d-41a6-4c33-8a4b-04079adf76c6",
   "metadata": {},
   "source": [
    "#### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b153271d-49ac-4521-8c31-56a33fe0fe99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "op ='sgd'\n",
      "cuda:0\n",
      "Epoch 1/5, Train Loss: 6.9649, Training Accuracy: 0.2918      Test Loss: 5.6637, Test Accuracy: 0.3682, Epoch Time: 2.9920, Dataloading Time: 0.1976, Training Time: 2.1341\n",
      "Epoch 2/5, Train Loss: 5.2776, Training Accuracy: 0.3876      Test Loss: 4.9375, Test Accuracy: 0.4198, Epoch Time: 2.9970, Dataloading Time: 0.2064, Training Time: 2.1047\n",
      "Epoch 3/5, Train Loss: 4.7164, Training Accuracy: 0.4260      Test Loss: 4.5933, Test Accuracy: 0.4457, Epoch Time: 2.9661, Dataloading Time: 0.2200, Training Time: 2.1056\n",
      "Epoch 4/5, Train Loss: 4.3641, Training Accuracy: 0.4443      Test Loss: 4.3878, Test Accuracy: 0.4575, Epoch Time: 3.0010, Dataloading Time: 0.2068, Training Time: 2.1243\n",
      "Epoch 5/5, Train Loss: 4.1139, Training Accuracy: 0.4598      Test Loss: 4.2019, Test Accuracy: 0.4715, Epoch Time: 3.1968, Dataloading Time: 0.2141, Training Time: 2.3192\n",
      "\n",
      "op ='sgd-nest'\n",
      "cuda:0\n",
      "Epoch 1/5, Train Loss: 6.7103, Training Accuracy: 0.3251      Test Loss: 5.4308, Test Accuracy: 0.4047, Epoch Time: 3.0795, Dataloading Time: 0.2206, Training Time: 2.1861\n",
      "Epoch 2/5, Train Loss: 5.1155, Training Accuracy: 0.4139      Test Loss: 4.8538, Test Accuracy: 0.4384, Epoch Time: 2.9246, Dataloading Time: 0.2062, Training Time: 2.0717\n",
      "Epoch 3/5, Train Loss: 4.6113, Training Accuracy: 0.4409      Test Loss: 4.4458, Test Accuracy: 0.4584, Epoch Time: 3.0463, Dataloading Time: 0.2191, Training Time: 2.1274\n",
      "Epoch 4/5, Train Loss: 4.2795, Training Accuracy: 0.4557      Test Loss: 4.2974, Test Accuracy: 0.4680, Epoch Time: 3.0646, Dataloading Time: 0.2257, Training Time: 2.1673\n",
      "Epoch 5/5, Train Loss: 4.0625, Training Accuracy: 0.4659      Test Loss: 4.1093, Test Accuracy: 0.4820, Epoch Time: 3.0982, Dataloading Time: 0.2200, Training Time: 2.1531\n",
      "\n",
      "op ='adagrad'\n",
      "cuda:0\n",
      "Epoch 1/5, Train Loss: 6.3021, Training Accuracy: 0.1628      Test Loss: 5.7364, Test Accuracy: 0.1784, Epoch Time: 2.9916, Dataloading Time: 0.2096, Training Time: 2.1391\n",
      "Epoch 2/5, Train Loss: 5.4991, Training Accuracy: 0.1753      Test Loss: 5.7595, Test Accuracy: 0.1784, Epoch Time: 3.1107, Dataloading Time: 0.2187, Training Time: 2.2043\n",
      "Epoch 3/5, Train Loss: 5.4454, Training Accuracy: 0.1760      Test Loss: 5.7350, Test Accuracy: 0.1784, Epoch Time: 3.1471, Dataloading Time: 0.2368, Training Time: 2.2035\n",
      "Epoch 4/5, Train Loss: 5.4247, Training Accuracy: 0.1761      Test Loss: 5.7071, Test Accuracy: 0.1784, Epoch Time: 3.2875, Dataloading Time: 0.2237, Training Time: 2.3931\n",
      "Epoch 5/5, Train Loss: 5.4127, Training Accuracy: 0.1776      Test Loss: 5.7505, Test Accuracy: 0.1784, Epoch Time: 3.4891, Dataloading Time: 0.2193, Training Time: 2.4639\n",
      "\n",
      "op ='adadelta'\n",
      "cuda:0\n",
      "Epoch 1/5, Train Loss: 10.0860, Training Accuracy: 0.1526      Test Loss: 9.0977, Test Accuracy: 0.2149, Epoch Time: 3.2419, Dataloading Time: 0.2493, Training Time: 2.3542\n",
      "Epoch 2/5, Train Loss: 8.4795, Training Accuracy: 0.2494      Test Loss: 7.4793, Test Accuracy: 0.2863, Epoch Time: 3.1379, Dataloading Time: 0.2345, Training Time: 2.2166\n",
      "Epoch 3/5, Train Loss: 7.2347, Training Accuracy: 0.2933      Test Loss: 6.8109, Test Accuracy: 0.3115, Epoch Time: 3.2074, Dataloading Time: 0.1990, Training Time: 2.3704\n",
      "Epoch 4/5, Train Loss: 6.6818, Training Accuracy: 0.3212      Test Loss: 6.4116, Test Accuracy: 0.3371, Epoch Time: 3.1525, Dataloading Time: 0.2264, Training Time: 2.2452\n",
      "Epoch 5/5, Train Loss: 6.3407, Training Accuracy: 0.3381      Test Loss: 6.1733, Test Accuracy: 0.3445, Epoch Time: 3.1021, Dataloading Time: 0.2196, Training Time: 2.2279\n",
      "\n",
      "op ='adam'\n",
      "cuda:0\n",
      "Epoch 1/5, Train Loss: 6.5787, Training Accuracy: 0.1528      Test Loss: 5.6889, Test Accuracy: 0.1784, Epoch Time: 3.1228, Dataloading Time: 0.2028, Training Time: 2.2230\n",
      "Epoch 2/5, Train Loss: 5.5051, Training Accuracy: 0.1770      Test Loss: 5.6923, Test Accuracy: 0.1784, Epoch Time: 3.0551, Dataloading Time: 0.2375, Training Time: 2.1659\n",
      "Epoch 3/5, Train Loss: 5.4842, Training Accuracy: 0.1769      Test Loss: 5.7044, Test Accuracy: 0.1784, Epoch Time: 3.2418, Dataloading Time: 0.3657, Training Time: 2.2093\n",
      "Epoch 4/5, Train Loss: 5.4820, Training Accuracy: 0.1754      Test Loss: 5.7088, Test Accuracy: 0.1497, Epoch Time: 3.2930, Dataloading Time: 0.3453, Training Time: 2.1460\n",
      "Epoch 5/5, Train Loss: 5.4691, Training Accuracy: 0.1769      Test Loss: 5.7021, Test Accuracy: 0.1784, Epoch Time: 3.0782, Dataloading Time: 0.2157, Training Time: 2.1721\n"
     ]
    }
   ],
   "source": [
    "opt = [\"sgd\",\"sgd-nest\",'adagrad','adadelta',\"adam\"]\n",
    "best_nw = 4\n",
    "train_loader = DataLoader(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=128,\n",
    "    collate_fn=data_collator,\n",
    "    num_workers = best_nw\n",
    ")\n",
    "train_accuracys = []\n",
    "# optim = torch.optim\n",
    "for op in opt:\n",
    "    print(f\"\\n{op =}\" )\n",
    "    #reset model\n",
    "    encoder = Encoder(vocab_size=tokenizer.vocab_size + 1,\n",
    "                  max_len=512,\n",
    "                  d_k=16,\n",
    "                  d_model=64,\n",
    "                  n_heads=4,\n",
    "                  n_layers=2,\n",
    "                  dropout_prob=0.1)\n",
    "    decoder = Decoder(vocab_size=tokenizer.vocab_size + 1,\n",
    "                      max_len=512,\n",
    "                      d_k=16,\n",
    "                      d_model=64,\n",
    "                      n_heads=4,\n",
    "                      n_layers=2,\n",
    "                      dropout_prob=0.1)\n",
    "    transformer = Transformer(encoder,decoder)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "    encoder.to(device)\n",
    "    decoder.to(device)\n",
    "    #set opt\n",
    "    if op == \"sgd\":\n",
    "        optimizer = torch.optim.SGD(transformer.parameters(), lr = 0.1, momentum = 0.9, weight_decay=5e-4)\n",
    "    elif op == \"sgd-nest\":\n",
    "        optimizer = torch.optim.SGD(transformer.parameters(), lr = 0.1, momentum = 0.9, weight_decay=5e-4, nesterov=True)\n",
    "    elif op == \"adagrad\":\n",
    "        optimizer = torch.optim.Adagrad(transformer.parameters(),lr=0.1)\n",
    "    elif op == \"adadelta\":\n",
    "        optimizer = torch.optim.Adadelta(transformer.parameters(),lr=0.1)\n",
    "    elif op == \"adam\":\n",
    "        optimizer = torch.optim.Adam(transformer.parameters(),lr = 0.1)\n",
    "        \n",
    "    train_losses, test_losses,time_epoch,time_dataloading,time_training,train_accuracy,test_accuracy = train(\n",
    "        transformer, criterion, optimizer, train_loader, valid_loader, epochs=5)\n",
    "\n",
    "    train_accuracys.append(train_accuracy[-1])\n",
    "\n",
    "# optimizer = torch.optim.Adam(transformer.parameters())\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eefa6ebe-15e8-4c4d-9398-b5785611a578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.float64(0.45982823457668465),\n",
       " np.float64(0.46590100619981706),\n",
       " np.float64(0.17758410407561745),\n",
       " np.float64(0.3381187112511434),\n",
       " np.float64(0.17692346783209675)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_accuracys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f7d0b3f-1986-455b-a31e-23a72e2f86f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArcAAAHbCAYAAADRQ7LLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABsoklEQVR4nO3de1gUZfsH8O/sLstBTgqCCiqKvqImYKLmWcvCUlPLQx4JzSwzMX7aq/UqmhYpZlpZUnlKE+1oFmYmaWqa5oEUSy0Rj6GCCIrIsjvP7w9jYd1dXFaQdfh+rmuv2ntmZ+9nn5n15tlnZiQhhAARERERkQKoqjoBIiIiIqKKwuKWiIiIiBSDxS0RERERKQaLWyIiIiJSDBa3RERERKQYLG6JiIiISDFY3BIRERGRYrC4JSIiIiLFYHFLRERERIrB4paoEkmShO7du1d1GlRJNm/ejE6dOqFmzZqQJAn9+/ev6pRsNnPmTEiShG3btpkte+edd9CyZUu4ublBkiQsXLjQpmV0d3Xv3h2SJFXa9lesWAFJkrBixYpKew+iysDilhRPkqRyPe5Vr732GiRJgpOTEzIzM6s6HcXLyMhAv379kJ6ejujoaMTFxeGpp566qzkUFx/FD5VKBU9PTzRq1Aj9+vXDu+++i8uXL5drm2vXrkVMTAycnZ0RExODuLg4PPDAA7dddi/JyMiAJEl4+umn7d7G1q1bMWTIENSvXx/Ozs6oVasWOnfujLfffhs3btyokDzL+gOEiKzTVHUCRJUtLi7OLLZw4ULk5uZaXFaR/vzzT7i5uVXqewCAEALLly+HJEnQ6/VYuXIl/vvf/1b6+1ZnW7ZswY0bN/DWW29h2LBhVZrLQw89hM6dOwMArl27hnPnzmHHjh3YsGED4uLikJiYiEGDBpm8ZsKECXjqqafQoEEDk/h3331n/G+9evVsXlZd6PV6vPDCC/jwww9Ro0YNPProo2jSpAlyc3OxefNmxMbGYsmSJUhOTkaTJk0qNZdPPvkE169fr7TtDxgwAA888ADq1q1bae9BVBlY3JLizZw50yy2YsUK5ObmWlxWkUJCQip1+8VSUlKQkZGBZ599FmvXrsWyZctY3Fay8+fPA4BDFHk9e/bE1KlTTWIGgwErV67EhAkTMHToUHh5eeGRRx4xLvf19YWvr6/ZtspqlyO1uapMmzYNH374Idq2bYuvv/4aAQEBxmUGgwGvvfYaXnvtNfTq1QsHDhyAp6dnpeVy6x8mFc3LywteXl6V+h5ElUIQVUMNGzYUt+7+J0+eFABEVFSU+OOPP0T//v1FrVq1BABx8uRJIYQQX331lXjqqadEcHCwcHV1FZ6enqJz587iiy++sPg+AES3bt1MYlFRUQKASE9PF4sWLRLNmjUTWq1WNGjQQMycOVMYDIZyt+epp54SAMS+ffvEmDFjBACxfft2q+unpqaKYcOGiYCAAKHVakWdOnVEZGSk2LBhg9m669evFw8//LCoVauWcHZ2Fg0bNhQjRowQhw8fNq7TrVs3s8/z1vYWf4ZCCLF8+XIBQCxfvlxs2LBBdOzYUbi7u4uGDRsKIYQoLCwU77zzjnjkkUdEYGCg0Gq1onbt2mLAgAHiwIEDVtt1u1yHDx8uAIg9e/ZYfP306dMFALFmzRqr71G8n1h6bN261bje4cOHxaBBg0Tt2rWFVqsVQUFBIiYmRmRlZZlts2HDhqJhw4YiJydHvPDCCyIwMFCo1WqxfPlyq3mU/hzj4+OtrrNixQoBQLRs2VLIsmyMx8XFmeRcvC1Lj7KWlfb777+LIUOGiDp16ggnJyfRoEEDMWHCBLM223KsCXGzPx988EHh7e0tnJ2dRcuWLUVCQoLQ6/UWP4fly5eLH374QXTo0EG4urqKWrVqiVGjRpm8f1ltKd1/lhw7dkyoVCpRq1YtkZmZaXW9YcOGCQBi+vTpJvHS/fzss88Kf39/4ezsLMLDw832ueJj6tZH8TFSeh1rn8WGDRtEu3bthKurq6hXr5743//+Z/x+WbFihQgNDRUuLi6ifv36Yt68eWbtKL2tYsXHs7XHrd93hYWF4q233hKtW7cWbm5uwt3dXXTu3Fl88803Zu9XvO0TJ06I+fPni+bNmwutViuioqKEEEIUFBSI+fPni9DQUOHp6Snc3NxEw4YNxaBBg0RqaqrV/qDqhyO3RLf4+++/8cADD6BVq1Z4+umnkZ2dDa1WC+DmqI1Wq0Xnzp1Rt25dXLp0CRs2bMDAgQPxzjvv4MUXX7T5faZMmYKff/4Zffr0QWRkJNavX4+ZM2dCp9Ph9ddft3k7ly9fxtdff40WLVqgTZs2GDVqFJYuXYqlS5eiS5cuZut/+eWXGDZsGIQQ6Nu3L5o1a4aLFy9iz549WLp0Kfr27Wtc9//+7/+wYMEC1KpVC/3794efnx/OnDmDLVu2oE2bNrjvvvtsztOSzz//HJs3b0afPn0wfvx45OXlGds0adIkdOnSBY899hhq1qyJ9PR0bNiwAd9//z22b9+Otm3bmmzLllzHjRuHTz/9FB9//DHatWtn8nqDwYDly5fDx8cHTzzxhNWcvb29ERcXh23btuHnn39GVFQUgoKCAMD43507dyIyMhI6nQ4DBw5EUFAQdu/ejUWLFuG7777Dr7/+ajZqWlhYiAcffBDXrl3D448/Do1GA39//zv6fAFg5MiRiIuLw5EjR5CWloZWrVpZXC88PBxxcXFYsWIFTp06ZTJlp6xlxTZs2IDBgwdDpVKhX79+qF+/Pv744w+89957+OGHH7Bnzx7UrFnT5DW3O9befPNNBAQE4IknnoCXlxd27NiBKVOmYM+ePfj8888t5pCcnIy+ffuiY8eO2L59Oz755BOcOHECO3fuNLYlJiYGixYtQlhYmMlJgMX9Z83KlSshyzKeffbZMvtm+vTpWLNmDZYtW4bXXnvNZJlOp0PPnj1x7do1jBw5Evn5+fjss88wbNgwZGVlGb9DiucD37qPeXt7l5ljsa+//hqbN29G//790alTJyQnJ2POnDkQQsDLywtz5sxBv3790L17d3z55Zd4+eWX4e/vj1GjRpW53f79+1v8nHbv3o3NmzebTMMqLCxEr169sG3bNoSHh2PMmDEoKipCcnKycU74hAkTzLb14osv4tdff0Xv3r3Rt29f+Pn5AQCioqLw2WefITQ0FNHR0XB2dsaZM2ewdetW/PbbbwgLC7Pps6FqoKqra6KqUNbILQAxY8YMi687ceKEWezq1auiVatWwsvLS+Tn55ssQxkjt40aNRLnz583xi9duiS8vb2Fh4eHKCwstLkt77zzjsnonSzLIigoSLi5uYnc3FyTdTMzM0WNGjVEjRo1LI6Anjlzxvj/3377rQAgWrVqZTbyVlRUZDJyZe/IrUqlEj/++KPZa27cuCHOnj1rFk9LSxPu7u6iZ8+eJvHy5NqiRQvh4eEhrl27ZrLed999JwCISZMmWWzHrW4d+SxmMBhEcHCwACA2bdpksmzKlCkCgBg9erRJvHh/jIyMFNevX7fp/YWwbeRWCCFGjhwpAIilS5feNv+y+tLasqysLOHp6SkCAgJERkaGybKkpCQBQEyYMMEYu92xtnnzZuPnUbqfZFkWzz33nABg8mtJ8eeg0WjEzp07jXG9Xi+6d+8uAIjdu3ebvX/xiKCtirdlaZ+9Vb169QQAcfr0aWOsuJ+7du1qcoyfOXNG+Pr6CmdnZ5P93lofFStr5NbJyUns3bvXGM/LyxN+fn7Czc1N1KlTx+S77PTp00Kr1YpWrVpZ3NbtfkE4evSo8Pb2FrVq1RLHjx83xl955RXjCHbpXw3y8vJERESE0Gq14ty5c8Z48XdFYGCgOHXqlMl7XLlyRUiSJNq0aWM2cq/X60VOTk6ZOVL1wqslEN2iTp06ePXVVy0ua9y4sVnM3d0dTz/9NHJzc/Hbb7/Z/D7Tp083OVHD19cX/fr1w9WrV3Hs2DGbt7N06VKoVCqMGDECwM2rQ4wYMQLXr1/H2rVrTdZduXIl8vPz8X//939o3bq12bYCAwON///+++8DABYtWgQfHx+T9SpqVLFfv37o2bOnWdzZ2dlkLmOxli1bokePHti+fTuKiorsynXcuHG4evWq2Wfz8ccfAwDGjh1rf4MA/PLLLzhx4gQeffRRREZGmiybMWMGatWqhTVr1kCn05m9dt68eXB1db2j97ekeI5sVlZWhW8buHliU15eHuLj49GwYUOTZU899RTuv/9+s88bsH6svffeewBgPGmrmCRJePPNNyFJEpKSksxeN2zYMHTq1Mn4XK1WIyoqCgDKdWxaU3wVkvr169923eJ1/vnnH7Nlb7zxhnGEGrh53MXExKCwsNDi52SPESNGmPy64eHhgT59+uD69et4/vnnTb7L6tevj86dO+OPP/6AXq8v1/tkZWWhd+/euH79Or7++ms0bdoUACDLMj744AMEBwdj1qxZJlei8fDwwIwZM6DT6fDVV1+ZbXPKlClm84klSYIQAi4uLlCpTEsXtVpt84g2VQ+clkB0i7CwMJN/eEq7ePEi3nzzTXz//fc4deoUCgoKTJYXn3BjizZt2pjFiovLK1eu2LSNffv24ffff8dDDz1kUpiOGjUKc+bMwdKlS/Hss88a43v37gUAkxOLrNm7dy+cnZ3RrVs3m3Kxx61TA0pLTU3FvHnzsHPnTmRmZpoUs8DNf1SL/zgoT66jRo3C1KlT8dFHH2HMmDEAgAsXLuC7775Dx44d0aJFiztoEXDw4EEAsHh9Y3d3d0RERGDz5s04duyYyRQBFxcXq1MGHN2vv/4KANizZw9OnDhhtvzGjRvIyspCVlaWyXQMa8far7/+iho1amDZsmUW38/V1RVHjx41i1fEMVXZNBoNOnToYBYvnkJUvP/cqfDwcLNY8fFibZnBYMCFCxcs/mFpSWFhIQYMGIATJ05gxYoV6Nq1q3HZsWPHkJOTg3r16mHWrFlmr7106RIAWOxHS98Lnp6eeOyxx7Bx40bcf//9GDRoELp37462bdvCycnJpnyp+mBxS3QLayOSly9fRtu2bXH69Gl06tQJPXv2hLe3N9RqNVJTU/HNN9+gsLDQ5vexdBa1RnPzkDQYDDZtY+nSpQBgNk+uadOmeOCBB/Drr7/iyJEjaNmyJQAgNzcXAGz6xys3NxcBAQFmoyQVydpnvWvXLjz44IMAbhbiTZs2hbu7OyRJwvr16/H777+bfNblydXb2xuDBw/GypUrkZaWhvvuuw8rVqyAXq+/41FbAMZ5w9baVlxgFK9XzM/Pr9Kus1z8R1ft2rUrZfvF19JdvHhxmevl5+ebFLdlHWt6vd5iUVR6W7eqiGOqLHXq1MHRo0dx5swZNGvWrMx1z5w5AwBml9Hy9fW1uJ8WfxbFx+idKuuzKGvZrX9ElmXMmDHYuXMnXnnlFeMIebHifeLIkSM4cuSI1W1Y6kdr+8Xnn3+ON954A2vWrDGO+Ht6eiI6OhpvvPHGXbnsIt0bOC2B6BbWCoylS5fi9OnTmD17Nnbu3Il3330Xs2fPxsyZM6vkQvYFBQXGn2ajoqLMbkZRPJpWXAADJSejnDt37rbb9/b2RmZmJmRZvu26xf9YW/pJs6x/rK191q+//joKCwuxZcsWbNiwAW+99RZmzZqFmTNnok6dOneUKwA899xzAICPPvoIwM3PyNPTE4MHD7bp9WUpLhwuXLhgcXnxT9u3FhiVVdjKsozt27cDgNlJeBWluC2HDx+GEMLq49YpC9ba7OnpCR8fnzK3dfLkyUppS1k6duwI4Oal98py9OhRnD9/HgEBAWZTGLKysizup8X7y71y6a1Zs2bh008/xaBBgzBnzhyz5cX7xJNPPllmPy5fvtzstdb2Czc3N8yZMwfp6elIT0/H0qVL0axZMyxatAgvvfRSxTaQ7mksbolsVPxza79+/cyW7dix426ngy+++AK5ubnGs5AtPVxcXLBq1Srj/M7in/s2b9582+23a9cOhYWF+Pnnn2+7bvFZ8LcWzbIs4/fffy9v03DixAnjHZ9Ku379Og4cOHBHuQLAAw88gNDQUKxevRqbN2/GX3/9heHDh1fIyE/xXGZLd5XKz8/Hvn374OrqetuRv4qyatUqnDp1Cq1atTKO4Fe09u3bA7h5xnxFbS87Oxt//fVXhWzvVmq1GkD5R3NHjRoFlUqFjz76yPizuiXFVzsZPXq02TK9Xm/xcyr+Dik9F97ePCtbUlISZs6ciXbt2mHlypUWi9HmzZvD09MT+/btK9dosK0aNWqE0aNH4+eff4a7uzs2bNhQ4e9B9y4Wt0Q2Kh51Kr6kULE1a9Zg48aNdz2f4hHZBQsW4OOPP7b4GDBgALKysoxf/FFRUXB3d8dbb72F1NRUs22WLk5feOEFAEBMTIzZLVz1er3JyGTxiOCt96BfsGCBXSNsDRs2RE5OjsnPmQaDAZMnT7ZYVJQn12Ljxo3D5cuXER0dDeDOTyQr1qlTJwQHB+P777/Hli1bTJbNmTMH2dnZGDp0qNV53RWl+NJmzz//PNRqNRYsWFBpo8PR0dHw8PDAq6++avEn6OvXrxt/SbDFxIkTAdwsDrOzs82WZ2Zm4s8//7Q735o1a0KSJOPUAVs1a9YMMTExyM7ORt++fc1OFpNlGbNnz8bq1asRHByMyZMnW9zOK6+8YnJC4dmzZ7Fo0SI4Ozub3MK5Vq1aAFDuPCvTrl27EB0djQYNGmDDhg1WT4DUaDR4/vnncerUKUyePNligZuWloaLFy/a9L6XLl1CWlqaWTwnJweFhYVwcXEpX0NI0TjnlshGI0eOxNy5c/Hiiy9i69ataNiwIX7//XekpKTgiSeesHjWb2X5+++/sX37dgQFBVk8calYdHQ0kpKSsHTpUgwcOBB+fn745JNP8NRTT6Fdu3Z4/PHH0axZM2RlZWHPnj0ICgrC+vXrAQCPPfYYJk+ejPnz56Np06YYMGAA/Pz8cO7cOaSkpGDy5MmYNGmS8X3mzZuHmTNnIjU1FcHBwdi3bx/S0tLQrVs3m0dUi7344ovYvHkzOnfujMGDB8PFxQXbtm3DuXPn0L17d7NR0fLkWmzEiBF4+eWXcf78ebRp08bi1SPsoVKpsGLFCkRGRuKxxx7DoEGD0LBhQ+zevRvbtm1DcHAw3nzzzQp5r2LFtwIGbhaSZ8+exfbt23Hu3DnUqlULq1atsnhViopSu3ZtJCUlYdCgQQgLC0OvXr0QEhKCwsJCZGRk4Oeff0bHjh2xadMmm7bXq1cvTJ8+HbNnz0aTJk3Qq1cvNGzYENnZ2fj777+xY8cOzJkzB82bN7crX3d3d7Rt2xbbt2/HyJEj0bRpU6hUKowcOdJs6sSt5s2bh9zcXCxbtgxNmzZF7969ERwcjLy8POOvAE2bNsXGjRstzm2tW7cu8vPzERoair59+xqvc5udnY133nnHZD58jx49IEkSXnnlFRw5cgReXl7w9va2eG3Yu+WZZ55BYWEh2rVrhw8++MBseVBQkPEavbNmzcKBAwfwzjvvIDk5GV27djUel4cPH8bvv/+O3bt3G69jW5Zz586hdevWCAsLQ2hoKAICApCdnY1vvvkGRUVFVv+QoGrqbl53jMhR3O4OZdakpqaKRx55RNSsWVN4eHiIbt26iS1btli9HiTKuM5t6eu+FrvddS2LTZs2TQAQcXFxZa5nMBhE/fr1hUqlMrne5sGDB8XgwYOFv7+/cHJyEnXr1hWPPvqo+O6778y28eWXX4oePXoILy8v4ezsLIKCgsTIkSNFWlqayXqpqanioYceEm5ubsLT01P069dP/PXXX7e9Q5k1X3zxhbj//vuFm5ub8PX1FYMHDxYnTpwo8/OzNddiI0aMEADEkiVLyvwcLbldXx06dEgMHDhQ+Pr6CicnJ9GwYUMRExMjLl26ZLZu8Z2ryuvWu21JkiTc3d1FUFCQ6Nu3r3j33XfF5cuXy5W/Pde5LXb06FExZswY0bBhQ6HVakXNmjVFq1atxMSJE02uuWrrdWZ//PFH0bdvX1G7dm3h5OQk6tSpIzp06CBmz55tsj+XtT9t3brV4rFy7Ngx8dhjjwlvb28hSZJNx92tuQ0aNEjUq1dPODk5CW9vb9GhQwfx1ltvWb1WcXE/X7582eQOZWFhYVbvirdixQrRqlUr4ezsXO47lN2qrH3W1uO0+LvT2uPW7zu9Xi8SExNFp06dhKenp3B2dhYNGjQQvXr1Eh988IHJdYzLOrZzcnLEzJkzRdeuXUXdunWFVqsV9erVE7169RLff/+9xc+Oqi9JCCEqt3wmInJMrVq1wsmTJ3H+/HmLo2xEFan4zl4ZGRlVmgeR0nHOLRFVS99//z3S0tIwfPhwFrZERArCObdEVK188MEHOHPmDD7++GO4uLhg6tSpVZ0SERFVIBa3RFStzJ07F2fPnkWzZs2wbNkyNGrUqKpTIiKiCuSQ0xIWL16MoKAguLi4oH379sZbhlqyYsUKs4vX85IgRGRNRkYG9Ho9jhw5gj59+lR1OlSNZGRkcL4t0V3gcMXtunXrEBsbi7i4OBw4cABhYWGIjIws81p4np6e+Oeff4yPU6dO3cWMiYiIiMhROFxxu2DBAowdOxbR0dFo0aIFlixZAjc3NyxbtszqayRJQp06dYwPa/elJiIiIiJlc6g5tzqdDvv378e0adOMMZVKhZ49e5Z5W8dr166hYcOGkGUZ999/P9544w2rt5ksLCxEYWGh8bksy7h8+TJ8fHwq7e49RERERGQ/IQSuXr2KevXqQaUqe2zWoYrbrKwsGAwGs5FXf39/HD161OJrik8KCQ0NRW5uLubPn4+OHTviyJEjCAwMNFs/Pj4es2bNqpT8iYiIiKjynDlzxmJ9V5pDFbf26NChAzp06GB83rFjRzRv3hyJiYmYPXu22frTpk1DbGys8Xlubi4aNGiAkydPGq91qVKpoFKpIMsyZFk2rlscNxgMKH3vC2txtVoNSZKg1+tNclCr1QBu3vvdlrhGo4EQwiQuSRLUarVZjtbibBPbxDaxTWwT28Q2sU33aptycnLQqFEjeHh44HYcqrj19fWFWq3GhQsXTOIXLlxAnTp1bNqGk5MTWrdujb///tvicmdnZzg7O5vFa9WqxQu5ExERETmg4qmjtkwhdagTyrRaLdq0aYOUlBRjTJZlpKSkmIzOlsVgMODw4cOoW7duZaVJRERERA7KoUZuASA2NhZRUVGIiIhAu3btsHDhQuTn5yM6OhoAMGrUKAQEBCA+Ph4A8Nprr+GBBx5AkyZNcOXKFSQkJODUqVN45plnqrIZRERERFQFHK64HTJkCC5duoQZM2YgMzMT4eHh2LRpk/Eks9OnT5ucJZeTk4OxY8ciMzMTNWvWRJs2bbBr1y60aNGiqppARERERFVEEqVn+VZDeXl58PLyQm5uLufcEhERETmg8tRrDjXnloiIiIjoTrC4JSIiIiLFYHFLRERERIrB4paIiIiIFIPFLREREREpBotbIiIiIlIMFrdEREREpBgsbokUZvHixQgKCoKLiwvat2+PvXv32vS6tWvXQpIk9O/f3yQuSZLFR0JCQiVkT0REdGdY3BIpyLp16xAbG4u4uDgcOHAAYWFhiIyMxMWLF8t8XUZGBiZPnowuXbqYLfvnn39MHsuWLYMkSXjyyScrqxlERER24x3KeIcyUpD27dujbdu2eO+99wAAsiyjfv36ePHFFzF16lSLrzEYDOjatStGjx6NHTt24MqVK1i/fr3V9+jfvz+uXr2KlJSUymgCERGRGd6hjKga0ul02L9/P3r27GmMqVQq9OzZE7t377b6utdeew1+fn4YM2bMbd/jwoULSE5OtmldIiKiqsDitpqr6PmZAPDnn3/i8ccfh5eXF2rUqIG2bdvi9OnTFZw53SorKwsGgwH+/v4mcX9/f2RmZlp8zc6dO7F06VJ89NFHNr3HypUr4eHhgSeeeOKO8yUiIqoMLG6rscqYn3nixAl07twZISEh2LZtGw4dOoTp06fDxcWlsppBdrp69SpGjhyJjz76CL6+vja9ZtmyZRg+fDj7k4iIHJamqhOgqrNgwQKMHTsW0dHRAIAlS5YgOTkZy5YtK3N+5vDhwzFr1izj/MzSXn31VTz22GOYN2+eMRYcHFxpbaASvr6+UKvVuHDhgkn8woULqFOnjtn6J06cQEZGBvr27WuMybIMANBoNDh27JhJ3+3YsQPHjh3DunXrKqkFREREd44jt9VUZczPlGUZycnJ+M9//oPIyEj4+fmhffv2ZZ6cRBVHq9WiTZs2Jid6ybKMlJQUdOjQwWz9kJAQHD58GKmpqcbH448/jh49eiA1NRX169c3WX/p0qVo06YNwsLCKr0tRERE9uLIbTVV1vzMo0ePWnxN8fzM1NRUi8svXryIa9eu4c0338ScOXMwd+5cbNq0CU888QS2bt2Kbt26VXQz6BaxsbGIiopCREQE2rVrh4ULFyI/P984Oj9q1CgEBAQgPj4eLi4uuO+++0xe7+3tDQBm8by8PHz++ed466237ko7iIiI7MXilmxiy/zM4p+0+/Xrh5deegkAEB4ejl27dmHJkiUsbu+CIUOG4NKlS5gxYwYyMzMRHh6OTZs2Gf+IOX36NFSq8v9gs3btWgghMHTo0IpOmYiIqEKxuK2mKmN+Zv369aHRaNCiRQuT1zZv3hw7d+6shFaQJRMmTMCECRMsLtu2bVuZr12xYoXF+LPPPotnn332DjMjIiKqfJxzW01VxvxMrVaLtm3b4tixYyavPX78OBo2bFjpbSIiIiLiyG01VhnzM6dMmYIhQ4aga9eu6NGjBzZt2oRvv/32tiOGSiDNkqo6hWpJxFXrmywSEdEtWNxWY5UxP3PAgAFYsmQJ4uPjMXHiRDRr1gxffvklOnfuXBlNICIiIjIhCSGq9bBHee5VTFQWjtxWDY7cEhEpX3nqNc65JSIiIiLFYHFLRERERIrBObdVQOKv11Wiek/AISIiqh44cktEREREisHiloiIiIgUg8UtERERESkGi1siIiIiUgwWt0RERESkGCxuiYiIiEgxWNwSERERkWKwuCUiIiIixWBxS0RERESKweKWiIiIiBSDxS0RERERKQaLWyIiIiJSDBa3RERERKQYLG6JiIiISDFY3BIRERGRYrC4JSIiIiLFYHFLRERERIrB4paIiIiIFIPFLREREREpBotbIiIiIlIMFrdEREREpBgsbomIiIhIMVjcEhEREZFisLglIiIiIsVgcUtEREREisHiloiIiIgUg8UtERERESkGi1siIiIiUgwWt0RERESkGCxuiYiIiEgxWNwSERERkWKwuCUiIiIixWBxS0RERESKweKWiIiIiBSDxS0RERERKQaLWyIiIiJSDBa3RERERKQYLG6JiIiISDFY3BIRERGRYrC4JSIiIiLFYHFLRERERIrhkMXt4sWLERQUBBcXF7Rv3x579+616XVr166FJEno379/5SZIRERERA7J4YrbdevWITY2FnFxcThw4ADCwsIQGRmJixcvlvm6jIwMTJ48GV26dLlLmRIRERGRo3G44nbBggUYO3YsoqOj0aJFCyxZsgRubm5YtmyZ1dcYDAYMHz4cs2bNQuPGje9itkRERETkSDRVnUBpOp0O+/fvx7Rp04wxlUqFnj17Yvfu3VZf99prr8HPzw9jxozBjh07ynyPwsJCFBYWGp/n5eUBAPR6PfR6vfE9VSoVZFmGLMsmuahUKhgMBgghbhtXq9WQJMm43WKSpIYQgFZruKX9akgS4OR0a1wDlUpAoymJCyGhqEgNlUqGRiObxdVqGWp1SVyWVdDrVdBoZKhUJXGDQQWDQQUnJwMkqSR3vV4FWbYUV0OWJWi1pm0qKnL8Nsly+fpJrVb/uz2DTXEAUEEFjVRyWAkIFIkiq3E11FBL6pIcIUMv9NBIGqhK/e1pEAYYYICT5AQJkjGuF3rIkK3GtZLWJL8iUQQBYRbXCR0kSHCSnMzijt6m4r60tZ80Gg2EECZxSZKgVqvNjnlr8cr+jijvvsc2sU1sE9uk9Dbdun5ZHKq4zcrKgsFggL+/v0nc398fR48etfianTt3YunSpUhNTbXpPeLj4zFr1iyz+MGDB1GjRg0AQO3atREcHIyTJ0/i0qVLxnUCAwMRGBiI48ePIzc31xhv3Lgx/Pz8kJaWhoKCAmM8JCQE3t7eOHjwoEln+fiEIi9PiylT9pnkkJAQAU9PHcaNO2SM6XRqJCS0RVBQLoYOLfkMsrJckZgYhtDQLPTunW6Mp6d7ISmpOTp1Oo8uXc4a46mptZGcHIzIyJMIDy9p044dgdi+PRADBx5H48YlbUpObozUVD+MHp0GX9+SNiUlhSA93RsxMQdNCtnERMdvU1ZW+fopNDQUWq0W+/aZtikiIgI6nQ6HDpW0qfjgC3INwtA6Q0vesygLiWcTEeoRit6+vUvaVJCOpMwkdPLuhC41S6bSpF5NRXJWMiJ9IhHuEV7Sppwd2H5lOwb6D0Rj15JfJ5KzkpF6NRWjA0bD18nXGE/KTEJ6QTpiGsRAqyopBhPPJiJPn4cpQVNM2pSQkQBPjSfGBY4zxnSyDgmnEhy+TQUFBeXqp7Zt2yI3N9fkO8XV1RVhYWHIyspCenrJvufl5YXmzZvj/PnzOHu2ZN+r7O+I8u57bBPbxDaxTUpv08GDB2ErSZQup6vY+fPnERAQgF27dqFDhw7G+Msvv4yff/4Ze/bsMVn/6tWrCA0Nxfvvv49HH30UAPD000/jypUrWL9+vcX3sDRyW79+fWRnZ8PT0xNA5f91otU6/iinEkduCwoq9y9jp9edHH6UU4kjtzem37i5vWo0gsE2sU1sE9tU3dqUk5MDHx8f5ObmGus1axyquNXpdHBzc8MXX3xhcsWDqKgoXLlyBd98843J+qmpqWjdurWx4QCMH7hKpcKxY8cQHBxc5nvm5eXBy8vLpg+rokjS7dehilfZe7o0ix1bFUScw3yFERFRJSlPveZQJ5RptVq0adMGKSkpxpgsy0hJSTEZyS0WEhKCw4cPIzU11fh4/PHH0aNHD6SmpqJ+/fp3M30iIiIiqmIONecWAGJjYxEVFYWIiAi0a9cOCxcuRH5+PqKjowEAo0aNQkBAAOLj4+Hi4oL77rvP5PXe3t4AYBYnIiIiIuVzuOJ2yJAhuHTpEmbMmIHMzEyEh4dj06ZNxpPMTp8+DZXKoQaciYiIiMhBONSc26rAObfVB+fcKhPn3BIRKd89O+eWiIiIiOhOsLglIiIiIsVgcUtEREREisHiloiIiIgUg8UtERERESkGi1siIiIiUgwWt0RERESkGCxuiYiIiEgxWNwSERERkWKwuCUiIiIixWBxS0RERESKweKWiIiIiBSDxS0RERERKQaLWyIiIiJSDBa3RERERKQYLG6JiIiISDFY3BIRERGRYrC4JSIiIiLFYHFLRERERIrB4paIiIiIFIPFLREREREpBotbIiIiIlIMFrdEREREpBgsbomIiIhIMVjcEhEREZFisLglIiIiIsVgcUtEREREisHiloiIiIgUg8UtERFRFVi8eDGCgoLg4uKC9u3bY+/evVbX/eqrrxAREQFvb2/UqFED4eHhWLVqldX1n3vuOUiShIULF1ZC5kSOjcUtERHRXbZu3TrExsYiLi4OBw4cQFhYGCIjI3Hx4kWL69eqVQuvvvoqdu/ejUOHDiE6OhrR0dH44YcfzNb9+uuv8euvv6JevXqV3Qwih8TiloiI6C5bsGABxo4di+joaLRo0QJLliyBm5sbli1bZnH97t27Y8CAAWjevDmCg4MRExOD0NBQ7Ny502S9c+fO4cUXX8Snn34KJyenu9EUIofD4paIiOgu0ul02L9/P3r27GmMqVQq9OzZE7t3777t64UQSElJwbFjx9C1a1djXJZljBw5ElOmTEHLli0rJXeie4GmqhMgIiKqTrKysmAwGODv728S9/f3x9GjR62+Ljc3FwEBASgsLIRarcb777+Phx9+2Lh87ty50Gg0mDhxYqXlTnQvYHFLRER0D/Dw8EBqaiquXbuGlJQUxMbGonHjxujevTv279+PRYsW4cCBA5AkqapTJapSLG6JiIjuIl9fX6jValy4cMEkfuHCBdSpU8fq61QqFZo0aQIACA8Px59//on4+Hh0794dO3bswMWLF9GgQQPj+gaDAf/3f/+HhQsXIiMjo1LaQuSIOOeWiIjoLtJqtWjTpg1SUlKMMVmWkZKSgg4dOti8HVmWUVhYCAAYOXIkDh06hNTUVOOjXr16mDJlisUrKhApGUduiYiI7rLY2FhERUUhIiIC7dq1w8KFC5Gfn4/o6GgAwKhRoxAQEID4+HgAQHx8PCIiIhAcHIzCwkJs3LgRq1atwgcffAAA8PHxgY+Pj8l7ODk5oU6dOmjWrNndbRxRFWNxS0REdJcNGTIEly5dwowZM5CZmYnw8HBs2rTJeJLZ6dOnoVKV/Lian5+P8ePH4+zZs3B1dUVISAhWr16NIUOGVFUTiByWJIQQVZ1EVcrLy4OXlxdyc3Ph6el5V96Tc/2rRmXv6dIsdmxVEHHV+iuMiKhaKE+9xjm3RERERKQYnJZARETVyixpVlWnUC3FibiqToGqCY7cEhEREZFisLglIiIiIsVgcUtEREREisHiloiIiIgUg8UtERERESkGi1siIiIiUgwWt0RERESkGCxuiYiIiEgx7Cpu161bhxs3blR0LkREREREd8Su4nbo0KGoU6cOxowZg61bt1Z0TkREREREdrGruN25cyeGDx+Ob7/9Fj179kSDBg0wdepUpKWlVXR+REREREQ2s6u47dixIxYvXozz58/jm2++QadOnfDee+8hLCwM4eHheOutt/DPP/9UdK5ERERERGW6oxPKNBoN+vTpg6SkJGRmZmLFihXw8fHByy+/jAYNGuDhhx/G6tWrodPpKipfIiIiIiKrKuxqCWlpadi7dy8OHz4MIQRCQkKQnZ2NUaNGITg4GDt37qyotyIiIiIisuiOitvjx48jLi4OTZs2RadOnfDZZ59h2LBh2LdvHw4fPowDBw5g7969qFWrFp577rmKypmIiIiIyCKNPS9atGgRPv30U+zfvx/Ozs7o27cvFi5ciF69ekGtVpusGxERgdjYWIwZM6ZCEiYiIiIissau4vall15Cp06dsGTJEgwePBheXl5lrh8REYHp06fblSARERERka3sKm5PnDiBRo0a2bx+y5Yt0bJlS3veioiIiIjIZnbNua1fvz7y8vKsLs/Ly4Ner7c7KSIiIiIie9hV3E6cOBEdO3a0urxTp074v//7P7uTIiIiIiKyh13F7aZNmzBw4ECrywcOHIiNGzfanRQRERERkT3sKm7Pnz+PgIAAq8vr1auHc+fO2Z0UEREREZE97CpufXx8cOzYMavL//zzT3h6etqdFBERERGRPewqbnv16oXExEQcPHjQbNmBAwfw4Ycf4tFHH73j5IiIiIiIysOuS4HNnj0bmzZtQrt27fD4448bL/OVlpaGb7/9Fn5+fpg9e3aFJkpEREREdDt2jdzWq1cP+/btw7Bhw5CSkoI5c+Zgzpw5+OmnnzB8+HD89ttvCAwMtDupxYsXIygoCC4uLmjfvj327t1rdd2vvvoKERER8Pb2Ro0aNRAeHo5Vq1bZ/d5EREREdO+ya+QWAOrWrYuVK1dCCIFLly4BAGrXrg1Jku4ooXXr1iE2NhZLlixB+/btsXDhQkRGRuLYsWPw8/MzW79WrVp49dVXERISAq1Wi++++w7R0dHw8/NDZGTkHeVCRERERPcWu0ZuS5MkCX5+fvDz87vjwhYAFixYgLFjxyI6OhotWrTAkiVL4ObmhmXLlllcv3v37hgwYACaN2+O4OBgxMTEIDQ0FDt37rzjXIiIiIjo3mL3yC0A/PLLLzhw4AByc3Mhy7LJMkmSMH369HJtT6fTYf/+/Zg2bZoxplKp0LNnT+zevfu2rxdC4KeffsKxY8cwd+5ci+sUFhaisLDQ+Lz4Tmt6vd54VzWVSgWVSgVZlk3aVRw3GAwQQtw2rlarIUmS2d3aJEkNIQCt1nBL+9WQJMDJ6da4BiqVgEZTEhdCQlGRGiqVDI1GNour1TLU6pK4LKug16ug0chQqUriBoMKBoMKTk4GSFJJ7nq9CrJsKa6GLEvQak3bVFTk+G2S5fL1k1qt/nd7BpviAKCCChqp5LASECgSRVbjaqihltQlOUKGXuihkTRQlfrb0yAMMMAAJ8kJEkr+iNQLPWTIVuNaSWuSX5EogoAwi+uEDhIkOElOZnFHb1NxX9raTxqNBkIIk7gkSVCr1WbHvLV4ZX9HlHffY5vK1yaoARgAyUlCqV0MQi8AuYy41nQARxQJQFiI6wQg/budW+MqQNKU3vi/27EWVwOSulRcvpmPpJFMhqeEQTh8mwwGQ7Xf99gm+9tUnjvf2lXcXr58Gb1798bevXshhIAkScbEi//fnuI2KysLBoMB/v7+JnF/f38cPXrU6utyc3MREBCAwsJCqNVqvP/++3j44YctrhsfH49Zs2aZxQ8ePIgaNWoAuDm9Ijg4GCdPnjROuQCAwMBABAYG4vjx48jNzTXGGzduDD8/P6SlpaGgoMAYDwkJgbe3Nw4ePGjSWT4+ocjL02LKlH0mOSQkRMDTU4dx4w4ZYzqdGgkJbREUlIuhQ0s+g6wsVyQmhiE0NAu9e6cb4+npXkhKao5Onc6jS5ezxnhqam0kJwcjMvIkwsNL2rRjRyC2bw/EwIHH0bhxSZuSkxsjNdUPo0enwde3pE1JSSFIT/dGTMxBk0I2MdHx25SVVb5+Cg0NhVarxb59pm2KiIiATqfDoUMlbSo++IJcgzC0ztCS9yzKQuLZRIR6hKK3b++SNhWkIykzCZ28O6FLzS4lbbqaiuSsZET6RCLcI7ykTTk7sP3Kdgz0H4jGro2N8eSsZKReTcXogNHwdfI1xpMyk5BekI6YBjHQqkqKwcSzicjT52FK0BSTNiVkJMBT44lxgeOMMZ2sQ8KpBIdvU0FBQbn6qW3btsjNzTX5TnF1dUVYWBiysrKQnl6y73l5eaF58+Y4f/48zp4t2fcq+zuivPse21S+Nnl38saV7VfgP9Afro1djfGs5CxcTb2KgNEBcPIt+UMvMykTBekFaBDTACptSUV5NvEs9Hl6BE0JMmlTRkIGNJ4aBI4rOfdE1sk4lXAKrkGuqDO0jjFelFWEs4ln4RHqAd/eJft7QXoBMpMy4d3JGzW71DTGr6ZeRVZyFnwifeAR7mGM5+zIcfg2HT9+vNrve2yT/W2ydIUuayRRupy20ZgxY7B27VosW7YM7du3R+PGjfHDDz+gUaNGePvtt7F79258//33ZkXq7RTfHGLXrl3o0KGDMf7yyy/j559/xp49eyy+TpZlpKen49q1a0hJScHs2bOxfv16dO/e3WxdSyO39evXR3Z2tvHavJX914lW6/ijnEocuS0oqNy/jJ1ed3L4UU4ljtzemH7j5vaq0QgG23RnbXrd5XWHH+VU4sjtq9dfrfb7Httkf5tycnLg4+OD3Nzc295Lwa7itm7duhg6dCgWLFiA7Oxs1K5dGz/++CMeeughAMATTzwBZ2dnJCUllWu7Op0Obm5u+OKLL9C/f39jPCoqCleuXME333xj03aeeeYZnDlzBj/88MNt183Ly4OXl5dNH1ZFqYCpyWSH8u/p5SPNYsdWBRFXyR1LijNLMv/1jipfnIir6hToHlaees2uE8quXLlivLatu7s7AODatWvG5Y888ohNheWttFot2rRpg5SUFGNMlmWkpKSYjOTejizLJqOzRERERFQ92DXntl69esjMzAQAODs7w8/PD7///jv69esHADh37pzdV06IjY1FVFQUIiIi0K5dOyxcuBD5+fmIjo4GAIwaNQoBAQGIj48HcHMObUREBIKDg1FYWIiNGzdi1apV+OCDD+x6fyIiIiK6d9lV3Hbt2hU//vgjXn31VQDAkCFDMG/ePOMcjeJr09pjyJAhuHTpEmbMmIHMzEyEh4dj06ZNxvm7p0+fhkpVMuCcn5+P8ePH4+zZs3B1dUVISAhWr16NIUOG2PX+RERERHTvsmvO7eHDh/Hjjz/ihRdegLOzM3JycjBo0CD89NNPAG4Wv0lJSahbt26FJ1zROOe2+uCcW2XinFsqL865rRqcc0t3ojz1ml0jt61atUKrVq2Mz2vWrIktW7bgypUrUKvV8PDwKOPVRERERESVo9wnlF2/fh1t2rTBkiVLzJZ5e3uzsCUiIiKiKlPu4tbNzQ0nT56skFvtEhERERFVJLsuBdarVy+7LvVFRERERFSZ7Cpup0+fjuPHj2PkyJHYuXMnzp07h8uXL5s9iIiIiIjuJrtOKCu+gcMff/yBNWvWWF3v1luoERERERFVJruK2xkzZnDOLRERERE5HLuK25kzZ1ZwGkREREREd86uObdERERERI7IrpHb11577bbrSJKE6dOn27N5IiIiIiK7VPi0BEmSIIRgcUtEREREd51d0xJkWTZ76PV6nDhxAi+99BIiIiJw8eLFis6ViIiIiKhMFTbnVqVSoVGjRpg/fz6aNm2KF198saI2TURERERkk0o5oaxr167YuHFjZWyaiIiIiMiqSilu9+3bB5WKF2IgIiIiorvLrhPKPvnkE4vxK1euYPv27fjqq6/wzDPP3FFiRERERETlZVdx+/TTT1td5uvri6lTp2LGjBn25kREREREZBe7ituTJ0+axSRJQs2aNeHh4XHHSRERERER2cOu4rZhw4YVnQcRERER0R2z66yvAwcO4P3337e6/P3330dqaqq9ORERERER2cWu4vbVV1/Fli1brC7/6aef8L///c/upIiIiIiI7GFXcbt//3506dLF6vIuXbpg3759didFRERERGQPu4rbq1evQqOxPl1XpVIhNzfX7qSIiIiIiOxhV3HbtGlTbN682eryTZs2oXHjxnYnRURERERkD7uK2zFjxiA5ORmxsbG4cuWKMX7lyhW89NJL2LRpE8aMGVNRORIRERER2cSuS4FNnDgRqampWLhwId555x3Uq1cPAHD+/HnIsoyRI0fipZdeqtBEiYiIiIhux67iVpIkLF++HKNGjcKXX36J9PR0AEC/fv3w5JNPonv37hWZIxERERGRTewqbov16NEDPXr0qKhciIiIiIjuiF1zbk+ePIlvv/3W6vJvv/0WGRkZ9uZERERERGQXu0ZuJ0+ejLy8PPTt29fi8sWLF8Pb2xtr1669o+SIiIiIiMrDrpHb3bt34+GHH7a6/KGHHsKOHTvsToqIiIiIyB52Fbc5OTnw8PCwutzd3R3Z2dl2J0VEREREZA+7itsGDRrgl19+sbp8x44dCAwMtDspIiIiIiJ72FXcDh06FElJSXjnnXcgy7IxbjAYsGjRIqxbtw7Dhg2rsCSJiIiIiGxh1wll06ZNw86dOzFp0iS8/vrraNasGQDg2LFjuHTpErp3745XX321QhMlIiIiIrodu0ZunZ2dsXnzZixduhTt2rVDVlYWsrKy0K5dOyxbtgxbtmyBs7NzRedKRERERFQmu2/ioFKpEB0djejoaIvL09LScN9999mdGBERERFRedk1cmvN2bNnkZCQgPDwcISFhVXkpomIiIiIbuuObr8LALm5ufj888/x6aefYseOHRBC4P7770dcXFxF5EdEREREZDO7iludTodvv/0Wn376Kb7//nsUFhZCkiRMnDgRU6ZMQb169So6TyIiIiKi2yrXtISffvoJY8aMgb+/PwYPHoyLFy9i/vz5xhHbLl26sLAlIiIioipj88htYGAg/vnnH7Ru3RqvvPIKnnrqKdSvXx8AcOLEiUpLkIiIiIjIVjYXt+fPn0ejRo0QHR2NQYMGwc/PrzLzIiIiIiIqN5unJSQnJ6NDhw6YOnUqAgIC8Mgjj2D58uXIzc2tzPyIiIiIiGxmc3H76KOPYvXq1bhw4QKWL18OjUaDcePGoU6dOhg9ejQkSTK5FS8RERER0d1W7uvcurm5YcSIEdi4cSPOnTuHuXPn4saNGxBCYMSIEXj44Yfx3nvvISMjoxLSJSIiIiKy7o5u4lC7dm1MnDgRe/bswfHjxzF16lScOnUKEydORHBwcEXlSERERERkkwq7Q1mTJk0wc+ZMHD9+HLt378aECRMqatNERERERDa54zuUWdK+fXu0b9++MjZNRERERGRVhY3cEhERERFVNRa3RERERKQYLG6JiIiISDFY3BIRERGRYrC4JSIiIiLFsPtqCQaDAT/88APS09ORk5MDIYTJckmSMH369DtOkIiIiIjIVnYVt/v27cOTTz6Js2fPmhW1xVjcEhEREdHdZte0hPHjx6OgoADr16/H5cuXIcuy2cNgMFR0rkREREREZbJr5PbQoUN4/fXX0bdv34rOh4iIiIjIbnaN3AYGBlqdjkBEREREVFXsKm7/+9//4qOPPkJeXl5F50NEREREZDe7piVcvXoV7u7uaNKkCZ566inUr18farXaZB1JkvDSSy9VSJJERERERLawq7idPHmy8f/fe+89i+uwuCUiIiKiu82u4vbkyZMVnQcRERER0R2zq7ht2LBhRedBRERERHTHePtdIiIiIlIMm0ZuGzVqBJVKhaNHj8LJyQmNGjWCJEllvkaSJJw4caJCkiQiIiIisoVNxW23bt0gSRJUKpXJ88qyePFiJCQkIDMzE2FhYXj33XfRrl07i+t+9NFH+OSTT5CWlgYAaNOmDd544w2r6xMRERGRctlU3K5YsaLM5xVp3bp1iI2NxZIlS9C+fXssXLgQkZGROHbsGPz8/MzW37ZtG4YOHYqOHTvCxcUFc+fOxSOPPIIjR44gICCg0vIkIiIiIsfjcHNuFyxYgLFjxyI6OhotWrTAkiVL4ObmhmXLlllc/9NPP8X48eMRHh6OkJAQfPzxx5BlGSkpKXc5cyIiIiKqanZdLaFYUVERjh49itzcXMiybLa8a9eu5dqeTqfD/v37MW3aNGNMpVKhZ8+e2L17t03buH79OoqKilCrVi2LywsLC1FYWGh8XnyXNb1eD71eb3xPlUoFWZZN2lUcNxgMJrcfthZXq9WQJMm43WKSpIYQgFZruKX9akgS4OR0a1wDlUpAoymJCyGhqEgNlUqGRiObxdVqGWp1SVyWVdDrVdBoZKhUJXGDQQWDQQUnJwMkqSR3vV4FWbYUV0OWJWi1pm0qKnL8Nsly+fqp+MYkBoPBpjgAqKCCRio5rAQEikSR1bgaaqilkhugyJChF3poJA1Upf72NAgDDDDASXKChJIpQXqhhwzZalwraU3yKxJFEBBmcZ3QQYIEJ8nJLO7obSruS1v7SaPRQAhhEpckCWq12uyYtxav7O+I8u57bFP52gQ1AAMgOUkotYtB6AUglxHXmk7HE0UCEBbiOgFI/27n1rgKkDSlN/7vdqzF1YCkLhWXb+YjaSST4SlhEA7fJoPBUO33PbbJ/jbdun5Z7CpuZVnGtGnT8P777+P69etW17P0j39ZsrKyYDAY4O/vbxL39/fH0aNHbdrGf//7X9SrVw89e/a0uDw+Ph6zZs0yix88eBA1atQAANSuXRvBwcE4efIkLl26ZFwnMDAQgYGBOH78OHJzc43xxo0bw8/PD2lpaSgoKDDGQ0JC4O3tjYMHD5p8Fj4+ocjL02LKlH0mOSQkRMDTU4dx4w4ZYzqdGgkJbREUlIuhQ0s+g6wsVyQmhiE0NAu9e6cb4+npXkhKao5Onc6jS5ezxnhqam0kJwcjMvIkwsNL2rRjRyC2bw/EwIHH0bhxSZuSkxsjNdUPo0enwde3pE1JSSFIT/dGTMxBk0I2MdHx25SVVb5+Cg0NhVarxb59pm2KiIiATqfDoUMlbSo++IJcgzC0ztCS9yzKQuLZRIR6hKK3b++SNhWkIykzCZ28O6FLzS4lbbqaiuSsZET6RCLcI7ykTTk7sP3Kdgz0H4jGro2N8eSsZKReTcXogNHwdfI1xpMyk5BekI6YBjHQqkqKwcSzicjT52FK0BSTNiVkJMBT44lxgeOMMZ2sQ8KpBIdvU0FBQbn6qW3btsjNzTX5TnF1dUVYWBiysrKQnl6y73l5eaF58+Y4f/48zp4t2fcq+zuivPse21S+Nnl38saV7VfgP9Afro1djfGs5CxcTb2KgNEBcPIt+UMvMykTBekFaBDTACptSUV5NvEs9Hl6BE0JMmlTRkIGNJ4aBI4LNMZknYxTCafgGuSKOkPrGONFWUU4m3gWHqEe8O1dsr8XpBcgMykT3p28UbNLTWP8aupVZCVnwSfSBx7hHsZ4zo4ch2/T8ePHq/2+xzbZ36aDBw/CVpIoXU7baM6cOZgxYwbGjRuHzp07Y+TIkZg7dy68vb3x/vvvQ5IkzJs3z2qBac358+cREBCAXbt2oUOHDsb4yy+/jJ9//hl79uwp8/Vvvvkm5s2bh23btiE0NNTiOpZGbuvXr4/s7Gx4enoCqPy/TrRaxx/lVOLIbUFB5f5l7PS6k8OPcipx5PbG9Bs3t1eNRjDYpjtr0+surzv8KKcSR25fvf5qtd/32Cb725STkwMfHx/k5uYa6zVr7Bq5XbFiBQYPHowPPvgA2dnZAG5epeDBBx9EVFQUOnTogJ9++qncxa2vry/UajUuXLhgEr9w4QLq1Klj5VU3zZ8/H2+++Sa2bNlitbAFAGdnZzg7O5vFNRoNNBrTj6O4Y25V/EHbGr91u8V9rNOZf/xCWI7LsmQlroJOZ55jcYF3K71eBUtTrYuKLOduLW4pF2txR2lTcVfa2k/2xGXI0AmdzXEDDDAI81849MLyzy9FoqhccUvvaS0uIMqVu6O0qfjKLeXpJ0mSLMatHfPljd/pd4Q9cbapHPF/d09RZHlsx2pcV464sBKXyxk3/Fu43rp5fTlzd4A2Fe8r1Xrf+xfbVDFtssauE8rOnj2LBx98EACMheKNGzdHT7RaLUaMGIFVq1aVe7tarRZt2rQxORms+OSw0iO5t5o3bx5mz56NTZs2ISIiotzvS0RERETKYNfIrY+PD65duwYAcHd3h6enp8l8DeDm8LE9YmNjERUVhYiICLRr1w4LFy5Efn4+oqOjAQCjRo1CQEAA4uPjAQBz587FjBkzsGbNGgQFBSEzM9OYl7u7u105EBEREdG9ya6R29atW+O3334zPu/RowcWLlyIX375BTt27MA777yDsLAwuxIaMmQI5s+fjxkzZiA8PBypqanYtGmT8SSz06dP459//jGu/8EHH0Cn02HgwIGoW7eu8TF//ny73p+IyBEtXrwYQUFBcHFxQfv27bF3716r6x45cgRPPvkkgoKCIEkSFi5caLaOwWDA9OnT0ahRI7i6uiI4OBizZ8+GHadhEFEpPFarnl0jt2PHjsXKlStRWFgIZ2dnvP766+jatSu6du0KIQRq1qyJpKQku5OaMGECJkyYYHHZtm3bTJ5nZGTY/T5ERPeC8t7c5vr162jcuDEGDRqEl156yeI2586diw8++AArV65Ey5YtsW/fPkRHR8PLywsTJ06s7CYRKRKPVcdg19USLMnNzcW2bdugVqvRsWNHq9eZdTR5eXnw8vKy6ey7ilKJdy6mMlT2H7nSLHZsVRBxyh+9aN++Pdq2bYv33nsPwM1zEerXr48XX3wRU6dOLfO1QUFBmDRpEiZNmmQS79OnD/z9/bF06VJj7Mknn4SrqytWr15d4W1wJLMk88tBUuWLE3FVnUKl47FaecpTr5V7WkJBQQFiY2Px7bffmsS9vLzQr18/9OnT554pbImIHF3xzW1KX32mvDe3saRjx45ISUnB8ePHAQC///47du7ciUcfffSOcyaqjnisOo5yT0twdXVFYmIiWrRoURn5EBFRKRVxcxtLpk6diry8PISEhECtVsNgMOD111/H8OHD7zRlomqJx6rjsGvObZs2bZCWllbRuRAR0V3y2Wef4dNPP8WaNWvQsmVLpKamYtKkSahXrx6ioqKqOj0i+heP1fKzq7hduHAhHnvsMdx33314+umny3VhXSIist2d3NymLFOmTMHUqVPx1FNPAQBatWqFU6dOIT4+nv9gEtmBx6rjsHnO7fbt2433Eo6KioJKpcK4cePg6emJpk2bIjQ01ORh76XAiIiohL03t7md69evm91lqPiWm0RUfjxWHYfNQ649evTA6tWrMXToUPj4+MDX1xfNmjWrzNyIiAjlv7mNTqfDH3/8Yfz/c+fOITU1Fe7u7mjSpAkAoG/fvnj99dfRoEEDtGzZEgcPHsSCBQswevToqmkkkQLwWHUMNhe3QgjjBYNvvdYsERFVniFDhuDSpUuYMWMGMjMzER4ebnZzm9IjO+fPn0fr1q2Nz+fPn4/58+ejW7duxu/vd999F9OnT8f48eNx8eJF1KtXD+PGjcOMGTPuatuIlITHqmOw+Tq3KpUKq1evxrBhwyo7p7uK17mtPnidW2WqDte5pYrF69xWjepwnVuqPJV2nVuJVRkRERERObByXeZgxIgRGDFihE3rSpIEvV5vV1JERA5hDf+grxLDOBpP5cTBt6pR2T+J2qlcxW3Pnj3xn//8p7JyISIiIiK6I+UqbqOiohQ355aIiIiIlKNcc26JiIiIiBwZi1siIiIiUgwWt0RERESkGDbPueVt3oiIiIjI0XHkloiIiIgUg8UtERERESkGi1siIiIiUgwWt0RERESkGCxuiYiIiEgxWNwSERERkWKwuCUiIiIixWBxS0RERESKweKWiIiIiBSDxS0RERERKQaLWyIiIiJSDBa3RERERKQYLG6JiIiISDFY3BIRERGRYrC4JSIiIiLFYHFLRERERIrB4paIiIiIFIPFLREREREpBotbIiIiIlIMFrdEREREpBgsbomIiIhIMVjcEhEREZFisLglIiIiIsVgcUtEREREisHiloiIiIgUg8UtERERESkGi1siIiIiUgwWt0RERESkGCxuiYiIiEgxWNwSERERkWKwuCUiIiIixWBxS0RERESKweKWiIiIiBSDxS0RERERKQaLWyIiIiJSDBa3RERERKQYLG6JiIiISDFY3BIRERGRYrC4JSIiIiLFYHFLRERERIrB4paIiIiIFIPFLREREREpBotbIiIiIlIMFrdEREREpBgsbomIiIhIMVjcEhEREZFisLglIiIiIsVgcUtEREREisHiloiIiIgUw+GK28WLFyMoKAguLi5o37499u7da3XdI0eO4Mknn0RQUBAkScLChQvvXqJERERE5HAcqrhdt24dYmNjERcXhwMHDiAsLAyRkZG4ePGixfWvX7+Oxo0b480330SdOnXucrZERERE5GgcqrhdsGABxo4di+joaLRo0QJLliyBm5sbli1bZnH9tm3bIiEhAU899RScnZ3vcrZERERE5Gg0VZ1AMZ1Oh/3792PatGnGmEqlQs+ePbF79+4Ke5/CwkIUFhYan+fl5QEA9Ho99Hq98X1VKhVkWYYsyyb5qFQqGAwGCCFuG1er1ZAkybjdYpKkhhCAVmswiet0akgS4OR0a1wDlUpAoymJCyGhqEgNlUqGRiObxdVqGWp1SVyWVdDrVdBoZKhUJXGDQQWDQQUnJwMkqSR3vV4FWbYUV0OWJWi1pm0qKnL8Nsly+fpJrVb/uz2DTXEAUEEFjVRyWAkIFIkiq3E11FBL6pIcIUMv9NBIGqhK/e1pEAYYYICT5AQJkjGuF3rIkK3GtZLWJL8iUQQBYRbXCR0kSHCSnMzijt6m4r60tZ80Gg2EECZxSZKgVqvNjnkJTlCjCDLUkFHSJhVkqKCHDA3kUm1SwQDVzVZBlMpdBT1UkM3iaughQYYepm1SowiAgMEsrgMgwQDTftJABwEVDKW+0iWIf3NXQbYYd+A26fXl6ycrcWvf5VADMACSk4RSqUPoBSCXEdeWCgIQRQIQFuI6AUj/bufWuAqQNKU3/u92rMXVgKQuFZdv5iNpJJPhKWEQDt8mg8FQrn4q17+5Wi3Uej0kWYZee8s+VlQECAHDrXGdDpAkGJxuOZ50OgiVCgZNqeNGCKiLiiCrVJAtxdVqyOpSx5MsQ6XXQ9ZoIKtKHU8GA1QGAwxOThBSqeNJr4dKls3iDt8mvb7CaqPb/Zt76/plcZjiNisrCwaDAf7+/iZxf39/HD16tMLeJz4+HrNmzTKLHzx4EDVq1AAA1K5dG8HBwTh58iQuXbpkXCcwMBCBgYE4fvw4cnNzjfHGjRvDz88PaWlpKCgoMMZDQkLg7e2NgwcPmnSWj08o8vK0mDJln0kOCQkR8PTUYdy4Q8aYTqdGQkJbBAXlYujQks8hK8sViYlhCA3NQu/e6cZ4eroXkpKao1On8+jS5awxnppaG8nJwYiMPInw8JI27dgRiO3bAzFw4HE0blzSpuTkxkhN9cPo0Wnw9S1pU1JSCNLTvRETc9CkkE1MdPw2ZWWVr59CQ0Oh1Wqxb59pmyIiIqDT6XDoUEmbig++INcgDK0ztOQ9i7KQeDYRoR6h6O3bu6RNBelIykxCJ+9O6FKzS0mbrqYiOSsZkT6RCPcIL2lTzg5sv7IdA/0HorFrY2M8OSsZqVdTMTpgNHydfI3xpMwkpBekI6ZBDLSqki+/xLOJyNPnYUrQFJM2JWQkwFPjiXGB44wxnaxDwqkEh29TQUFBufqpbdu2yM3NNflecXV1RVhYGLKyspCeXrLveTkNRPOiJJxXd8JZTUmbahtSEaxPxklNJC6pS9oUqN+BQMN2HHcaiFxVSZsa65PhZ0hFmnY0CqSSNoUUJcFbTsdB5xiToi9UlwityMM+Z9N+iihMgE7yxCFtST+poUPbwgTkqoJw1Kmkn1xFFsJ0ichShyJdU9JPXnK647fp4MHy9ZOXF5o3b47z58/j7NmS7whr3+XenbxxZfsV+A/0h2tjV2M8KzkLV1OvImB0AJx8S4qDzKRMFKQXoEFMA6i0JYXK2cSz0OfpETQlyKRNGQkZ0HhqEDgu0BiTdTJOJZyCa5Ar6gwtmUZXlFWEs4ln4RHqAd/eJZ9jQXoBMpMy4d3JGzW71DTGr6ZeRVZyFnwifeAR7mGM5+zIcfg2HT9+vFz9VK5/c6dMQUhSErzT03EwJsak6AtNTIQ2Lw/7ptyy7yUkQOfpiUPjSu17Oh3aJiQgNygIR4eWOp6yshCWmIis0FCk9y51PKWno3lSEs536oSzXUodT6mpCE5OxsnISFwKDy9p044dCNy+HccHDkRu41LHU3Iy/FJTkTZ6NAp8Sx1Pjt6mffsqrDa63b+5Bw8ehK0kUbqcrkLnz59HQEAAdu3ahQ4dOhjjL7/8Mn7++Wfs2bOnzNcHBQVh0qRJmDRpUpnrWRq5rV+/PrKzs+Hp6Qmg8kdutVrHH+VU4shtQUHljtw6ve7k8KOcShy5vTH9xs3tVcbI7Wdujj/KCQWO3A7Or9SR29ddXnf4UU4ljty+ev3Vyhu5rVHD8Uc5ocCR2/z8uzZym5OTAx8fH+Tm5hrrNWscZuTW19cXarUaFy5cMIlfuHChQk8Wc3Z2tjg/V6PRQKMx/TiKO+ZW6lKdbUv81u0W97FOZ/7xC2E5LsuSlbgKOp15jsUF3q30ehUsTbUuKrKcu7W4pVysxR2lTcVdaWs/2ROXIUMndDbHDTDAIMynN+iF5Z9fikRRueKW3tNaXECUK3dHaZP07z8E5eknSZIsxs2P+Zs5FBd4ZutDb/HEBTUs524troHlfrIcFxbjEmSL8ZtFq6W4A7fp376xvZ/KGf+32aLI8tiO1biuHHFhJS6XM274t3C9dfP6cubuAG0q/u4tb//Z9G+urmT/0eis7HuW4kJYjEuybDGukmWoLMX/LVrN4norx1ORlePJStxh21Tq+LzT2sjeuCUOc0KZVqtFmzZtkJKSYozJsoyUlBSTkVwiIiIiImscZuQWAGJjYxEVFYWIiAi0a9cOCxcuRH5+PqKjowEAo0aNQkBAAOLj4wHcPAntjz/+MP7/uXPnkJqaCnd3dzRp0qTK2kFEREREVcOhitshQ4bg0qVLmDFjBjIzMxEeHo5NmzYZTzI7ffq0yU8W58+fR+vWrY3P58+fj/nz56Nbt27Ytm3b3U6fiIiIiKqYQxW3ADBhwgRMmDDB4rJbC9agoCA4yPlwREREROQAHGbOLRERERHRnWJxS0RERESKweKWiIiIiBSDxS0RERERKQaLWyIiIiJSDBa3RERERKQYLG6JiIiISDFY3BIRERGRYrC4JSIiIiLFYHFLRERERIrB4paIiIiIFIPFLREREREpBotbIiIiIlIMFrdEREREpBgsbomIiIhIMVjcEhEREZFisLglIiIiIsVgcUtEREREisHiloiIiIgUg8UtERERESkGi1siIiIiUgwWt0RERESkGCxuiYiIiEgxWNwSERERkWKwuCUiIiIixWBxS0RERESKweKWiIiIiBSDxS0RERERKQaLWyIiIiJSDBa3RERERKQYLG6JiIiISDFY3BIRERGRYrC4JSIiIiLFYHFLRERERIrB4paIiIiIFIPFLREREREpBotbIiIiIlIMFrdEREREpBgsbomIiIhIMVjcEhEREZFisLglIiIiIsVgcUtEREREisHiloiIiIgUg8UtERERESkGi1siIiIiUgwWt0RERESkGCxuiYiIiEgxWNwSERERkWKwuCUiIiIixWBxS0RERESKweKWiIiIiBSDxS0RERERKQaLWyIiIiJSDBa3RERERKQYLG6JiIiISDFY3BIRERGRYrC4JSIiIiLFYHFLRERERIrB4paIiIiIFIPFLREREREpBotbIiIiIlIMFrdEREREpBgsbomIiIhIMVjcEhEREZFisLglIiIiIsVgcUtEREREiuGQxe3ixYsRFBQEFxcXtG/fHnv37i1z/c8//xwhISFwcXFBq1atsHHjxruUKRERERE5EocrbtetW4fY2FjExcXhwIEDCAsLQ2RkJC5evGhx/V27dmHo0KEYM2YMDh48iP79+6N///5IS0u7y5kTERERUVVzuOJ2wYIFGDt2LKKjo9GiRQssWbIEbm5uWLZsmcX1Fy1ahF69emHKlClo3rw5Zs+ejfvvvx/vvffeXc6ciIiIiKqapqoTKE2n02H//v2YNm2aMaZSqdCzZ0/s3r3b4mt2796N2NhYk1hkZCTWr19vcf3CwkIUFhYan+fm5gIALl++DL1eb3xPlUoFWZYhy7JJLiqVCgaDAUKI28bVajUkSTJut4QaAODkZDCJFhVZi2sgSQIaTUlcCAl6vRqSJEOjkc3iKpUMtbokLssqGAwqqNUyVKqSuMGggiyroNEYIEkluev1KghhKa6GEBKcnEzbZD13x2nTlSvl6ye1Wv3v9gw2xXEDkCBBI5UcVgICeqG3GldBBbWkLmkTZBiEAWpJDVWpvz0NwgAZMjSSBhIkY1wv9BAQVuNOkpNJikWiCADKFXf0NhUfw7b2k0ajgRDCJC5JEtRqtdkxL13XQA09ZKggo6RNKshQwQAZasil2qSCASrIMEADUSp3FfRQQZjF1dBDgoAepm1S42Z/GGyMa1AEAQmGUl/pEsS/uUuQLcYduE2XL5evn6zErX2X31DdAGRA0kgolTqEXgCijLhTqSAAUXTze6Rccenf7RuD/27fWlwFSOpScRkQBnEzVmp4ShiEw7cpJyenXP1Urn9znZyg1ushCQG90y37WNG/+5iNcU1REYQkwaApddwIAbVeD1mSIFuKq1SQ1aWOJ1mGymCArFZDVpU6ngwGqGQZBo0GQip1POn1UAlhFnf4Nl2+XGG10e3+zc3JyQEAk21ZJRzIuXPnBACxa9cuk/iUKVNEu3btLL7GyclJrFmzxiS2ePFi4efnZ3H9uLg4AYAPPvjggw8++OCDj3vscebMmdvWkw41cns3TJs2zWSkV5ZlXL58GT4+PpAkqYxXUl5eHurXr48zZ87A09OzqtOhCsJ+VR72qTKxX5WHfWo7IQSuXr2KevXq3XZdhypufX19oVarceHCBZP4hQsXUKdOHYuvqVOnTrnWd3Z2hrOzs0nM29vb/qSrIU9PTx6ECsR+VR72qTKxX5WHfWobLy8vm9ZzqBPKtFot2rRpg5SUFGNMlmWkpKSgQ4cOFl/ToUMHk/UB4Mcff7S6PhEREREpl0ON3AJAbGwsoqKiEBERgXbt2mHhwoXIz89HdHQ0AGDUqFEICAhAfHw8ACAmJgbdunXDW2+9hd69e2Pt2rXYt28fPvzww6psBhERERFVAYcrbocMGYJLly5hxowZyMzMRHh4ODZt2gR/f38AwOnTp6EqdeZhx44dsWbNGvzvf//DK6+8gqZNm2L9+vW47777qqoJiuXs7Iy4uDizaR10b2O/Kg/7VJnYr8rDPq0ckhC2XFOBiIiIiMjxOdScWyIiIiKiO8HiloiIiIgUg8UtERERESkGi1uqFN27d8ekSZOqOg36F/vj3pSRkQFJkpCamlrVqVi1YsUKXivcRhXVn08//TT69+9v8/rbtm2DJEm4cuXKHb0v3Zl74XhWCha3RHRXlPcfZCKqHPyDhJSOxS0RUTWn0+mqOgUiogrD4pYAAF988QVatWoFV1dX+Pj4oGfPnsjPz4der8fEiRPh7e0NHx8f/Pe//0VUVJTJCFx+fj5GjRoFd3d31K1bF2+99VbVNUQhqqI/JEnCxx9/jAEDBsDNzQ1NmzbFhg0bTNZJS0vDo48+Cnd3d/j7+2PkyJHIysq6bd4zZ87EypUr8c0330CSJEiShG3btlXER3VP2bRpEzp37mzsvz59+uDEiRPG5Xv37kXr1q3h4uKCiIgIHDx40OT1BoMBY8aMQaNGjeDq6opmzZph0aJFJuvYso90794dEyZMwKRJk+Dr64vIyEgAwIIFC9CqVSvUqFED9evXx/jx43Ht2jWT7a9YsQINGjSAm5sbBgwYgOzs7Ar+lO4dd6M/DQYDYmNjje/x8ssv49YreMqyjPj4eON2wsLC8MUXX1jMedu2bYiOjkZubq7xWJw5cyYAYNWqVYiIiICHhwfq1KmDYcOG4eLFixXwSSnT3ej/4l+83njjDfj7+8Pb2xuvvfYa9Ho9pkyZglq1aiEwMBDLly+/K22+Zwiq9s6fPy80Go1YsGCBOHnypDh06JBYvHixuHr1qpgzZ46oVauW+Oqrr8Sff/4pnnvuOeHp6Sn69etnfP3zzz8vGjRoILZs2SIOHTok+vTpIzw8PERMTEyVteleVlX9AUAEBgaKNWvWiL/++ktMnDhRuLu7i+zsbCGEEDk5OaJ27dpi2rRp4s8//xQHDhwQDz/8sOjRo8dt87569aoYPHiw6NWrl/jnn3/EP//8IwoLCyvrI3RYX3zxhfjyyy/FX3/9JQ4ePCj69u0rWrVqJQwGg7h69aqoXbu2GDZsmEhLSxPffvutaNy4sQAgDh48KIQQQqfTiRkzZojffvtNpKeni9WrVws3Nzexbt0643vYso9069ZNuLu7iylTpoijR4+Ko0ePCiGEePvtt8VPP/0kTp48KVJSUkSzZs3E888/b3zdr7/+KlQqlZg7d644duyYWLRokfD29hZeXl534+NzOHejP+fOnStq1qwpvvzyS/HHH3+IMWPGCA8PD5P+nDNnjggJCRGbNm0SJ06cEMuXLxfOzs5i27ZtQgghtm7dKgCInJwcUVhYKBYuXCg8PT2Nx+LVq1eFEEIsXbpUbNy4UZw4cULs3r1bdOjQQTz66KN37fO819yN/o+KihIeHh7ihRdeEEePHhVLly4VAERkZKR4/fXXxfHjx8Xs2bOFk5OTOHPmTBV9Eo6HxS2J/fv3CwAiIyPDbJm/v79ISEgwPtfr9aJBgwbGL9arV68KrVYrPvvsM+M62dnZwtXVlcWtnaqqPwCI//3vf8bn165dEwDE999/L4QQYvbs2eKRRx4xec2ZM2cEAHHs2LEy8xbi5pd06X+QSYhLly4JAOLw4cMiMTFR+Pj4iIKCAuPyDz74wOQfQ0teeOEF8eSTTxqf324fEeJmcdu6devb5vf5558LHx8f4/OhQ4eKxx57zGSdIUOGVNvi9laV0Z9169YV8+bNMz4vKioSgYGBxv68ceOGcHNzE7t27TLZzpgxY8TQoUOFEKbFrRBCLF++3KY+++233wQAY/FLZauM/o+KihINGzYUBoPBGGvWrJno0qWL8blerxc1atQQSUlJFdugexinJRDCwsLw0EMPoVWrVhg0aBA++ugj5OTkIDc3FxcuXEC7du2M66rVarRp08b4/MSJE9DpdGjfvr0xVqtWLTRr1uyutkFJKrs/3njjDbi7uxsfp0+fNi4LDQ01/n+NGjXg6elp/Fny999/x9atW01eGxISYnxfa3lTib/++gtDhw5F48aN4enpiaCgIAA3byv+559/IjQ0FC4uLsb1O3ToYLaNxYsXo02bNqhduzbc3d3x4YcfGvvQln2kmKXYli1b8NBDDyEgIAAeHh4YOXIksrOzcf36dQDAn3/+abJvWcuxurgb/fnPP/+YfOYajQYRERHG53///TeuX7+Ohx9+2OTY/OSTT0x+IrfF/v370bdvXzRo0AAeHh7o1q2bsT1krrL7v1jLli2hUpWUa/7+/mjVqpXxuVqtho+PD6eQlMLilqBWq/Hjjz/i+++/R4sWLfDuu++iWbNmyMjIqOrUqqXK7o/nnnsOqampxke9evWMy5ycnEzWlSQJsiwDAK5du4a+ffuavDY1NRV//fUXunbtajXvkydPVkjeStC3b19cvnwZH330Efbs2YM9e/YAsP2ErrVr12Ly5MkYM2YMNm/ejNTUVERHR9t1QliNGjVMnmdkZKBPnz4IDQ3Fl19+if3792Px4sXlyq+6cYT+LJ4TnZycbHJc/vHHH1bn3VqSn5+PyMhIeHp64tNPP8Vvv/2Gr7/+ulztqW7uVv9b+l4u67uaWNzSvyRJQqdOnTBr1iwcPHgQWq0WKSkp8Pf3x2+//WZcz2Aw4MCBA8bnwcHBcHJyMh7UAJCTk4Pjx4/f1fyVpjL7o1atWmjSpInxodFobMrp/vvvx5EjRxAUFGTy+iZNmhgLJUt5F/8DqdVqYTAY7uhzuZdlZ2fj2LFj+N///oeHHnoIzZs3NxnZbt68OQ4dOoQbN24YY7/++qvJNn755Rd07NgR48ePR+vWrdGkSROT0TkvL6/b7iPW7N+/H7Is46233sIDDzyA//znPzh//rzJOs2bNzfZtyzlWF3crf6sW7euyWeu1+uxf/9+4/MWLVrA2dkZp0+fNjsu69evbzF3S8fi0aNHkZ2djTfffBNdunRBSEgIRwLLcDf6n+zH4pawZ88evPHGG9i3bx9Onz6Nr776CpcuXULz5s3x4osvIj4+Ht988w2OHTuGmJgY5OTkQJIkAIC7uzvGjBmDKVOm4KeffkJaWhqefvppk59QqHwctT9eeOEFXL58GUOHDsVvv/2GEydO4IcffkB0dDQMBkOZeQNAUFAQDh06hGPHjiErKwtFRUV3nNO9pGbNmvDx8cGHH36Iv//+Gz/99BNiY2ONy4cNGwZJkjB27Fj88ccf2LhxI+bPn2+yjaZNm2Lfvn344YcfcPz4cUyfPt2kkAVw233EmiZNmqCoqAjvvvsu0tPTsWrVKixZssRknYkTJ2LTpk2YP38+/vrrL7z33nvYtGnTHX4y96a71Z8xMTF48803sX79ehw9ehTjx483uRmDh4cHJk+ejJdeegkrV67EiRMncODAAbz77rtYuXKlxdyDgoJw7do1pKSkICsrC9evX0eDBg2g1WqN/b9hwwbMnj274j4whblb/U92qupJv1T1/vjjDxEZGSlq164tnJ2dxX/+8x/x7rvvCiFunrwwYcIE4enpKWrWrCn++9//ikGDBomnnnrK+PqrV6+KESNGCDc3N+Hv7y/mzZsnunXrxhPK7FRV/QFAfP311yYxLy8vsXz5cuPz48ePiwEDBghvb2/h6uoqQkJCxKRJk4Qsy2XmLYQQFy9eFA8//LBwd3cXAMTWrVvv9KO65/z444+iefPmwtnZWYSGhopt27aZfO67d+8WYWFhQqvVivDwcPHll1+anIBy48YN8fTTTwsvLy/h7e0tnn/+eTF16lQRFhZmfA9b9hFr+8OCBQtE3bp1haurq4iMjBSffPKJyYlIQtw8oz4wMFC4urqKvn37ivnz51fbE8ruVn/GxMQIT09P4e3tLWJjY8WoUaNMThCUZVksXLhQNGvWTDg5OYnatWuLyMhI8fPPPwshzE8oE0KI5557Tvj4+AgAIi4uTgghxJo1a0RQUJBwdnYWHTp0EBs2bLjtCVDV2d3of0sn4lo6fhs2bCjefvvtSmvrvUYS4pYL5hGVQZZlNG/eHIMHD+Zf9Q6A/UG3w32EiKob2ybbUbV16tQpbN68Gd26dUNhYSHee+89nDx5EsOGDavq1Kol9gfdDvcRIqruODGSyqRSqbBixQq0bdsWnTp1wuHDh7FlyxbjPEq6u9gfdDvcR4iouuO0BCIiIiJSDI7cEhEREZFisLglIiIiIsVgcUtEREREisHiloiIiIgUg8UtERERESkGi1siIge2YsUKSJKEjIyMCtvmzJkzb3s7XiKiexWLWyKicjpy5AhGjBiBgIAAODs7o169ehg+fDiOHDli9zbfeOMNrF+/vuKSJCKqpnidWyKicvjqq68wdOhQ1KpVC2PGjEGjRo2QkZGBpUuXIjs7G2vXrsWAAQPKvV13d3cMHDgQK1asMIkbDAYUFRXB2dm5wkZb9Xo99Ho9XFxcKmR7RESOhMUtEZGNTpw4gdDQUDRo0ADbt29H7dq1jcuysrLQpUsXnDlzBocOHULjxo3LtW1rxa0SyLIMnU7HYpqI7gpOSyAislFCQgKuX7+ODz/80KSwBQBfX18kJiYiPz8f8+bNA1Ayt/Xo0aMYPHgwPD094ePjg5iYGNy4ccP4WkmSkJ+fj5UrV0KSJEiShKeffhqA5Tm3QUFB6NOnD7Zt24aIiAi4urqiVatW2LZtG4Cbo8utWrWCi4sL2rRpg4MHD5rkeuuc26efftr4vrc+Zs6caVyvsLAQcXFxaNKkCZydnVG/fn28/PLLKCwsNNm+JEmYMGECPv30U7Rs2RLOzs7YtGkTAGDt2rVo06YNPDw84OnpiVatWmHRokV29QcRkSWaqk6AiOhe8e233yIoKAhdunSxuLxr164ICgpCcnKySXzw4MEICgpCfHw8fv31V7zzzjvIycnBJ598AgBYtWoVnnnmGbRr1w7PPvssACA4OLjMXP7++28MGzYM48aNw4gRIzB//nz07dsXS5YswSuvvILx48cDAOLj4zF48GAcO3YMKpXl8Yxx48ahZ8+eJrFNmzbh008/hZ+fH4Cbo6+PP/44du7ciWeffRbNmzfH4cOH8fbbb+P48eNm84V/+uknfPbZZ5gwYQJ8fX0RFBSEH3/8EUOHDsVDDz2EuXPnAgD+/PNP/PLLL4iJiSmzvURENhNERHRbV65cEQBEv379ylzv8ccfFwBEXl6eiIuLEwDE448/brLO+PHjBQDx+++/G2M1atQQUVFRZttbvny5ACBOnjxpjDVs2FAAELt27TLGfvjhBwFAuLq6ilOnThnjiYmJAoDYunWrMVaclzV//fWX8PLyEg8//LDQ6/VCCCFWrVolVCqV2LFjh8m6S5YsEQDEL7/8YowBECqVShw5csRk3ZiYGOHp6WncJhFRZeC0BCIiG1y9ehUA4OHhUeZ6xcvz8vKMsRdeeMFknRdffBEAsHHjRrvzadGiBTp06GB83r59ewDAgw8+iAYNGpjF09PTbdpufn4+BgwYgJo1ayIpKQlqtRoA8Pnnn6N58+YICQlBVlaW8fHggw8CALZu3WqynW7duqFFixYmMW9vb+Tn5+PHH38sZ2uJiGzHaQlERDYoLlqLi1xrLBXBTZs2NVknODgYKpXqjq5dW7qABQAvLy8AQP369S3Gc3JybNru2LFjceLECezatQs+Pj7G+F9//YU///zTbK5xsYsXL5o8b9Sokdk648ePx2effYZHH30UAQEBeOSRRzB48GD06tXLptyIiGzB4paIyAZeXl6oW7cuDh06VOZ6hw4dQkBAADw9Pa2uUxGX9CoeUbU1Lmy4MM6iRYuQlJSE1atXIzw83GSZLMto1aoVFixYYPG1txbVrq6uZuv4+fkhNTUVP/zwA77//nt8//33WL58OUaNGoWVK1feNj8iIluwuCUislGfPn3w0UcfYefOnejcubPZ8h07diAjIwPjxo0zif/1118mI5l///03ZFlGUFCQMVbVdwzbsWMHJk+ejEmTJmH48OFmy4ODg/H777/joYceuqNctVot+vbti759+0KWZYwfPx6JiYmYPn06mjRpcidNICICwEuBERHZbMqUKXB1dcW4ceOQnZ1tsuzy5ct47rnn4ObmhilTppgsW7x4scnzd999FwDw6KOPGmM1atTAlStXKifx2/jnn38wePBgdO7cGQkJCRbXGTx4MM6dO4ePPvrIbFlBQQHy8/Nv+z63fmYqlQqhoaEAYHY5MSIie3HklojIRk2bNsXKlSsxfPhwtGrVyuwOZVlZWUhKSjK7jNfJkyfx+OOPo1evXti9ezdWr16NYcOGISwszLhOmzZtsGXLFixYsAD16tVDo0aNjCeDVbaJEyfi0qVLePnll7F27VqTZaGhoQgNDcXIkSPx2Wef4bnnnsPWrVvRqVMnGAwGHD16FJ999hl++OEHRERElPk+zzzzDC5fvowHH3wQgYGBOHXqFN59912Eh4ejefPmldlEIqpGWNwSEZXDoEGDEBISgvj4eGNB6+Pjgx49euCVV17BfffdZ/aadevWYcaMGZg6dSo0Gg0mTJhgNkK6YMECPPvss/jf//6HgoICREVF3bXi9tKlSzAYDIiNjTVbFhcXh9DQUKhUKqxfvx5vv/02PvnkE3z99ddwc3ND48aNERMTg//85z+3fZ8RI0bgww8/xPvvv48rV66gTp06GDJkCGbOnGn1GrxEROXF2+8SEVWSmTNnYtasWbh06RJ8fX2rOh0iomqBfyoTERERkWKwuCUiIiIixWBxS0RERESKwTm3RERERKQYHLklIiIiIsVgcUtEREREisHiloiIiIgUg8UtERERESkGi1siIiIiUgwWt0RERESkGCxuiYiIiEgxWNwSERERkWL8P45PqWhd72FwAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "bars = plt.bar(opt, train_accuracys, color=['blue', 'green', 'orange', 'purple', 'red'])\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Optimizers', fontsize=12)\n",
    "plt.ylabel('Train Accuracy', fontsize=12)\n",
    "plt.title('Train Accuracy for Different Optimizers', fontsize=14)\n",
    "\n",
    "# Annotate bars with values\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, height, f'{height:.2f}', \n",
    "             ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Show plot\n",
    "plt.ylim(0, 0.5)  # Adjust the y-axis for better visibility\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d985860-1c5c-4f40-88c1-e6d6e48cfc16",
   "metadata": {},
   "source": [
    "Using sgd-nest for best result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "70337563-7aca-4fac-8d91-d55f7613f761",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(transformer.parameters(), lr = 0.1, momentum = 0.9, weight_decay=5e-4, nesterov=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bcecc0-5434-4356-9918-b80a62b745be",
   "metadata": {},
   "source": [
    "### Exploring Torch.compile (need to rework)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "26d92727-be84-498d-adb7-4b8e49c5c622",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torch.utils\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "89fbe450-05dc-43f3-a0ea-9521c7d90504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Torch compile training---\n"
     ]
    },
    {
     "ename": "TorchRuntimeError",
     "evalue": "Failed running call_function <built-in function add>(*(FakeTensor(..., device='cuda:0', size=(32, s1, 64),\n           grad_fn=<EmbeddingBackward0>), FakeTensor(..., device='cuda:0', size=(1, 512, 64))), **{}):\nThe size of tensor a (s1) must match the size of tensor b (512) at non-singleton dimension 1)\n\nfrom user code:\n   File \"/tmp/ipykernel_1096/1340379155.py\", line 8, in forward\n    enc_output = self.encoder(enc_input, enc_mask)\n  File \"/home/wei/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/ipykernel_1096/300891069.py\", line 28, in forward\n    x = self.pos_encoding(x)\n  File \"/home/wei/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/ipykernel_1096/1597975147.py\", line 16, in forward\n    x = x + self.pe[:, :x.size(1), :]\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTorchRuntimeError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 41\u001b[0m\n\u001b[1;32m     36\u001b[0m transformer\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mDataParallel(transformer)\n\u001b[1;32m     37\u001b[0m cudnn\u001b[38;5;241m.\u001b[39mbenchmark \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m train_losses, test_losses,time_epoch,time_dataloading,time_training,train_accuracy,test_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m epoch_times\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mmean(time_epoch))\n",
      "Cell \u001b[0;32mIn[64], line 36\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, criterion, optimizer, train_loader, valid_loader, epochs)\u001b[0m\n\u001b[1;32m     33\u001b[0m dec_mask \u001b[38;5;241m=\u001b[39m dec_mask\u001b[38;5;241m.\u001b[39mmasked_fill(dec_input \u001b[38;5;241m==\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mpad_token_id, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43menc_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdec_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdec_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m), targets)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Backward and optimize\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:184\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m     module_kwargs \u001b[38;5;241m=\u001b[39m ({},)\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[1;32m    186\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_apply(replicas, inputs, module_kwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:433\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    428\u001b[0m saved_dynamic_layer_stack_depth \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    429\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mget_dynamic_layer_stack_depth()\n\u001b[1;32m    430\u001b[0m )\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;66;03m# Restore the dynamic layer stack depth if necessary.\u001b[39;00m\n\u001b[1;32m    436\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mpop_dynamic_layer_stack_and_undo_to_depth(\n\u001b[1;32m    437\u001b[0m         saved_dynamic_layer_stack_depth\n\u001b[1;32m    438\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:1116\u001b[0m, in \u001b[0;36mCatchErrorsWrapper.__call__\u001b[0;34m(self, frame, cache_entry, frame_state)\u001b[0m\n\u001b[1;32m   1110\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m hijacked_callback(\n\u001b[1;32m   1111\u001b[0m                 frame, cache_entry, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhooks, frame_state\n\u001b[1;32m   1112\u001b[0m             )\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m compile_lock, _disable_current_modes():\n\u001b[1;32m   1115\u001b[0m     \u001b[38;5;66;03m# skip=1: skip this frame\u001b[39;00m\n\u001b[0;32m-> 1116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_torchdynamo_orig_callable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m   1118\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:948\u001b[0m, in \u001b[0;36mConvertFrame.__call__\u001b[0;34m(self, frame, cache_entry, hooks, frame_state, skip)\u001b[0m\n\u001b[1;32m    946\u001b[0m counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframes\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    947\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 948\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inner_convert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m    950\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    951\u001b[0m     counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframes\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mok\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    952\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:472\u001b[0m, in \u001b[0;36mConvertFrameAssert.__call__\u001b[0;34m(self, frame, cache_entry, hooks, frame_state, skip)\u001b[0m\n\u001b[1;32m    458\u001b[0m compile_id \u001b[38;5;241m=\u001b[39m CompileId(frame_id, frame_compile_id)\n\u001b[1;32m    460\u001b[0m signpost_event(\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdynamo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_convert_frame_assert._compile\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    469\u001b[0m     },\n\u001b[1;32m    470\u001b[0m )\n\u001b[0;32m--> 472\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_globals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_locals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_builtins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_torchdynamo_orig_callable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_one_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_export\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_export_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompile_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompile_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_utils_internal.py:84\u001b[0m, in \u001b[0;36mcompile_time_strobelight_meta.<locals>.compile_time_strobelight_meta_inner.<locals>.wrapper_function\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m     83\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 84\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mStrobelightCompileTimeProfiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofile_compile_time\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphase_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_strobelight/compile_time_profiler.py:129\u001b[0m, in \u001b[0;36mStrobelightCompileTimeProfiler.profile_compile_time\u001b[0;34m(cls, func, phase_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprofile_compile_time\u001b[39m(\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28mcls\u001b[39m, func: Any, phase_name: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[1;32m    127\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39menabled:\n\u001b[0;32m--> 129\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mprofiler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    132\u001b[0m         logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprofiler is not set\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 79\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:817\u001b[0m, in \u001b[0;36m_compile\u001b[0;34m(code, globals, locals, builtins, compiler_fn, one_graph, export, export_constraints, hooks, cache_entry, cache_size, frame, frame_state, compile_id, skip)\u001b[0m\n\u001b[1;32m    815\u001b[0m guarded_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 817\u001b[0m     guarded_code \u001b[38;5;241m=\u001b[39m \u001b[43mcompile_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    818\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m guarded_code\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[1;32m    820\u001b[0m     Unsupported,\n\u001b[1;32m    821\u001b[0m     TorchRuntimeError,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    828\u001b[0m     BisectValidationException,\n\u001b[1;32m    829\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/utils.py:231\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (dynamo_timed)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    230\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 231\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m     time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[1;32m    233\u001b[0m compilation_time_metrics[key]\u001b[38;5;241m.\u001b[39mappend(time_spent)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:636\u001b[0m, in \u001b[0;36m_compile.<locals>.compile_inner\u001b[0;34m(code, one_graph, hooks, transform)\u001b[0m\n\u001b[1;32m    634\u001b[0m CompileContext\u001b[38;5;241m.\u001b[39mget()\u001b[38;5;241m.\u001b[39mattempt \u001b[38;5;241m=\u001b[39m attempt\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 636\u001b[0m     out_code \u001b[38;5;241m=\u001b[39m \u001b[43mtransform_code_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mRestartAnalysis \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py:1185\u001b[0m, in \u001b[0;36mtransform_code_object\u001b[0;34m(code, transformations, safe)\u001b[0m\n\u001b[1;32m   1182\u001b[0m instructions \u001b[38;5;241m=\u001b[39m cleaned_instructions(code, safe)\n\u001b[1;32m   1183\u001b[0m propagate_line_nums(instructions)\n\u001b[0;32m-> 1185\u001b[0m \u001b[43mtransformations\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m clean_and_assemble_instructions(instructions, keys, code_options)[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:178\u001b[0m, in \u001b[0;36mpreserve_global_state.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m cleanup \u001b[38;5;241m=\u001b[39m setup_compile_debug()\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    180\u001b[0m     cleanup\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:582\u001b[0m, in \u001b[0;36m_compile.<locals>.transform\u001b[0;34m(instructions, code_options)\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tracing(tracer\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mtracing_context), tracer\u001b[38;5;241m.\u001b[39mset_current_tx():\n\u001b[0;32m--> 582\u001b[0m         \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mUnspecializeRestartAnalysis:\n\u001b[1;32m    584\u001b[0m     speculation_log\u001b[38;5;241m.\u001b[39mclear()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:2451\u001b[0m, in \u001b[0;36mInstructionTranslator.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2450\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 2451\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:893\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    892\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mpush_tx(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 893\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    894\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    895\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:805\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_block_stack(inst)\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 805\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43minst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopcode\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mshould_exit\n\u001b[1;32m    807\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObservedException:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:499\u001b[0m, in \u001b[0;36mbreak_graph_if_unsupported.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_graph_break(\u001b[38;5;28mself\u001b[39m, inst, speculation\u001b[38;5;241m.\u001b[39mreason)\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported \u001b[38;5;28;01mas\u001b[39;00m excp:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneric_context_manager_depth \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    502\u001b[0m         \u001b[38;5;66;03m# We don't support graph break under GenericContextWrappingVariable,\u001b[39;00m\n\u001b[1;32m    503\u001b[0m         \u001b[38;5;66;03m# If there is, we roll back to the checkpoint and fall back.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1459\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.CALL_FUNCTION\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   1457\u001b[0m args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpopn(inst\u001b[38;5;241m.\u001b[39margval)\n\u001b[1;32m   1458\u001b[0m fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpop()\n\u001b[0;32m-> 1459\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:743\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.call_function\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inner_fn \u001b[38;5;129;01mand\u001b[39;00m callable(inner_fn) \u001b[38;5;129;01mand\u001b[39;00m is_forbidden(inner_fn):\n\u001b[1;32m    742\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempt to trace forbidden callable \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minner_fn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 743\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpush(\u001b[43mfn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/variables/nn_module.py:437\u001b[0m, in \u001b[0;36mNNModuleVariable.call_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m istype(fn, types\u001b[38;5;241m.\u001b[39mFunctionType)\n\u001b[0;32m--> 437\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minline_user_function_return\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvariables\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUserFunctionVariable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn_source\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:749\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.inline_user_function_return\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minline_user_function_return\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, args, kwargs):\n\u001b[1;32m    746\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;124;03m    A call to some user defined function by inlining it.\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mInliningInstructionTranslator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minline_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:2666\u001b[0m, in \u001b[0;36mInliningInstructionTranslator.inline_call\u001b[0;34m(cls, parent, func, args, kwargs)\u001b[0m\n\u001b[1;32m   2663\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   2664\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minline_call\u001b[39m(\u001b[38;5;28mcls\u001b[39m, parent, func, args, kwargs):\n\u001b[1;32m   2665\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m patch\u001b[38;5;241m.\u001b[39mdict(counters, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munimplemented\u001b[39m\u001b[38;5;124m\"\u001b[39m: counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minline_call\u001b[39m\u001b[38;5;124m\"\u001b[39m]}):\n\u001b[0;32m-> 2666\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minline_call_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:2782\u001b[0m, in \u001b[0;36mInliningInstructionTranslator.inline_call_\u001b[0;34m(parent, func, args, kwargs)\u001b[0m\n\u001b[1;32m   2780\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2781\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m strict_ctx:\n\u001b[0;32m-> 2782\u001b[0m         \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2783\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObservedException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   2784\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObserved exception DURING INLING \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:893\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    892\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mpush_tx(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 893\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    894\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    895\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:805\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_block_stack(inst)\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 805\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43minst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopcode\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mshould_exit\n\u001b[1;32m    807\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObservedException:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:499\u001b[0m, in \u001b[0;36mbreak_graph_if_unsupported.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_graph_break(\u001b[38;5;28mself\u001b[39m, inst, speculation\u001b[38;5;241m.\u001b[39mreason)\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported \u001b[38;5;28;01mas\u001b[39;00m excp:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneric_context_manager_depth \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    502\u001b[0m         \u001b[38;5;66;03m# We don't support graph break under GenericContextWrappingVariable,\u001b[39;00m\n\u001b[1;32m    503\u001b[0m         \u001b[38;5;66;03m# If there is, we roll back to the checkpoint and fall back.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1500\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.CALL_FUNCTION_EX\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;66;03m# Map to a dictionary of str -> VariableTracker\u001b[39;00m\n\u001b[1;32m   1499\u001b[0m kwargsvars \u001b[38;5;241m=\u001b[39m kwargsvars\u001b[38;5;241m.\u001b[39mkeys_as_python_constant()\n\u001b[0;32m-> 1500\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margsvars\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargsvars\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:743\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.call_function\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inner_fn \u001b[38;5;129;01mand\u001b[39;00m callable(inner_fn) \u001b[38;5;129;01mand\u001b[39;00m is_forbidden(inner_fn):\n\u001b[1;32m    742\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempt to trace forbidden callable \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minner_fn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 743\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpush(\u001b[43mfn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:344\u001b[0m, in \u001b[0;36mUserMethodVariable.call_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39mvalue, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m invoke_and_store_as_constant(tx, fn, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_name(), args, kwargs)\n\u001b[0;32m--> 344\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:293\u001b[0m, in \u001b[0;36mUserFunctionVariable.call_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_constant:\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m invoke_and_store_as_constant(\n\u001b[1;32m    290\u001b[0m         tx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_name(), args, kwargs\n\u001b[1;32m    291\u001b[0m     )\n\u001b[0;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:90\u001b[0m, in \u001b[0;36mBaseUserFunctionVariable.call_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_function\u001b[39m(\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28mself\u001b[39m, tx, args: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mList[VariableTracker]\u001b[39m\u001b[38;5;124m\"\u001b[39m, kwargs: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDict[str, VariableTracker]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     89\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVariableTracker\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minline_user_function_return\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:749\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.inline_user_function_return\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minline_user_function_return\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, args, kwargs):\n\u001b[1;32m    746\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;124;03m    A call to some user defined function by inlining it.\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mInliningInstructionTranslator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minline_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:2666\u001b[0m, in \u001b[0;36mInliningInstructionTranslator.inline_call\u001b[0;34m(cls, parent, func, args, kwargs)\u001b[0m\n\u001b[1;32m   2663\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   2664\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minline_call\u001b[39m(\u001b[38;5;28mcls\u001b[39m, parent, func, args, kwargs):\n\u001b[1;32m   2665\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m patch\u001b[38;5;241m.\u001b[39mdict(counters, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munimplemented\u001b[39m\u001b[38;5;124m\"\u001b[39m: counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minline_call\u001b[39m\u001b[38;5;124m\"\u001b[39m]}):\n\u001b[0;32m-> 2666\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minline_call_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:2782\u001b[0m, in \u001b[0;36mInliningInstructionTranslator.inline_call_\u001b[0;34m(parent, func, args, kwargs)\u001b[0m\n\u001b[1;32m   2780\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2781\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m strict_ctx:\n\u001b[0;32m-> 2782\u001b[0m         \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2783\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObservedException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   2784\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObserved exception DURING INLING \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:893\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    892\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mpush_tx(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 893\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    894\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    895\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:805\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_block_stack(inst)\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 805\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43minst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopcode\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mshould_exit\n\u001b[1;32m    807\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObservedException:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:499\u001b[0m, in \u001b[0;36mbreak_graph_if_unsupported.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_graph_break(\u001b[38;5;28mself\u001b[39m, inst, speculation\u001b[38;5;241m.\u001b[39mreason)\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported \u001b[38;5;28;01mas\u001b[39;00m excp:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneric_context_manager_depth \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    502\u001b[0m         \u001b[38;5;66;03m# We don't support graph break under GenericContextWrappingVariable,\u001b[39;00m\n\u001b[1;32m    503\u001b[0m         \u001b[38;5;66;03m# If there is, we roll back to the checkpoint and fall back.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1459\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.CALL_FUNCTION\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   1457\u001b[0m args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpopn(inst\u001b[38;5;241m.\u001b[39margval)\n\u001b[1;32m   1458\u001b[0m fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpop()\n\u001b[0;32m-> 1459\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:743\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.call_function\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inner_fn \u001b[38;5;129;01mand\u001b[39;00m callable(inner_fn) \u001b[38;5;129;01mand\u001b[39;00m is_forbidden(inner_fn):\n\u001b[1;32m    742\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempt to trace forbidden callable \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minner_fn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 743\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpush(\u001b[43mfn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/variables/nn_module.py:437\u001b[0m, in \u001b[0;36mNNModuleVariable.call_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m istype(fn, types\u001b[38;5;241m.\u001b[39mFunctionType)\n\u001b[0;32m--> 437\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minline_user_function_return\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvariables\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUserFunctionVariable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn_source\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:749\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.inline_user_function_return\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minline_user_function_return\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, args, kwargs):\n\u001b[1;32m    746\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;124;03m    A call to some user defined function by inlining it.\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mInliningInstructionTranslator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minline_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:2666\u001b[0m, in \u001b[0;36mInliningInstructionTranslator.inline_call\u001b[0;34m(cls, parent, func, args, kwargs)\u001b[0m\n\u001b[1;32m   2663\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   2664\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minline_call\u001b[39m(\u001b[38;5;28mcls\u001b[39m, parent, func, args, kwargs):\n\u001b[1;32m   2665\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m patch\u001b[38;5;241m.\u001b[39mdict(counters, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munimplemented\u001b[39m\u001b[38;5;124m\"\u001b[39m: counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minline_call\u001b[39m\u001b[38;5;124m\"\u001b[39m]}):\n\u001b[0;32m-> 2666\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minline_call_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:2782\u001b[0m, in \u001b[0;36mInliningInstructionTranslator.inline_call_\u001b[0;34m(parent, func, args, kwargs)\u001b[0m\n\u001b[1;32m   2780\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2781\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m strict_ctx:\n\u001b[0;32m-> 2782\u001b[0m         \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2783\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObservedException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   2784\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObserved exception DURING INLING \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:893\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    892\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mpush_tx(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 893\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    894\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    895\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:805\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_block_stack(inst)\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 805\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43minst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopcode\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mshould_exit\n\u001b[1;32m    807\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObservedException:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:499\u001b[0m, in \u001b[0;36mbreak_graph_if_unsupported.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_graph_break(\u001b[38;5;28mself\u001b[39m, inst, speculation\u001b[38;5;241m.\u001b[39mreason)\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported \u001b[38;5;28;01mas\u001b[39;00m excp:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneric_context_manager_depth \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    502\u001b[0m         \u001b[38;5;66;03m# We don't support graph break under GenericContextWrappingVariable,\u001b[39;00m\n\u001b[1;32m    503\u001b[0m         \u001b[38;5;66;03m# If there is, we roll back to the checkpoint and fall back.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1500\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.CALL_FUNCTION_EX\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;66;03m# Map to a dictionary of str -> VariableTracker\u001b[39;00m\n\u001b[1;32m   1499\u001b[0m kwargsvars \u001b[38;5;241m=\u001b[39m kwargsvars\u001b[38;5;241m.\u001b[39mkeys_as_python_constant()\n\u001b[0;32m-> 1500\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margsvars\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargsvars\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:743\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.call_function\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inner_fn \u001b[38;5;129;01mand\u001b[39;00m callable(inner_fn) \u001b[38;5;129;01mand\u001b[39;00m is_forbidden(inner_fn):\n\u001b[1;32m    742\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempt to trace forbidden callable \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minner_fn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 743\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpush(\u001b[43mfn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:344\u001b[0m, in \u001b[0;36mUserMethodVariable.call_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39mvalue, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m invoke_and_store_as_constant(tx, fn, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_name(), args, kwargs)\n\u001b[0;32m--> 344\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:293\u001b[0m, in \u001b[0;36mUserFunctionVariable.call_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_constant:\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m invoke_and_store_as_constant(\n\u001b[1;32m    290\u001b[0m         tx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_name(), args, kwargs\n\u001b[1;32m    291\u001b[0m     )\n\u001b[0;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:90\u001b[0m, in \u001b[0;36mBaseUserFunctionVariable.call_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_function\u001b[39m(\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28mself\u001b[39m, tx, args: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mList[VariableTracker]\u001b[39m\u001b[38;5;124m\"\u001b[39m, kwargs: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDict[str, VariableTracker]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     89\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVariableTracker\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minline_user_function_return\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:749\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.inline_user_function_return\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minline_user_function_return\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, args, kwargs):\n\u001b[1;32m    746\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;124;03m    A call to some user defined function by inlining it.\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mInliningInstructionTranslator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minline_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:2666\u001b[0m, in \u001b[0;36mInliningInstructionTranslator.inline_call\u001b[0;34m(cls, parent, func, args, kwargs)\u001b[0m\n\u001b[1;32m   2663\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   2664\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minline_call\u001b[39m(\u001b[38;5;28mcls\u001b[39m, parent, func, args, kwargs):\n\u001b[1;32m   2665\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m patch\u001b[38;5;241m.\u001b[39mdict(counters, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munimplemented\u001b[39m\u001b[38;5;124m\"\u001b[39m: counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minline_call\u001b[39m\u001b[38;5;124m\"\u001b[39m]}):\n\u001b[0;32m-> 2666\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minline_call_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:2782\u001b[0m, in \u001b[0;36mInliningInstructionTranslator.inline_call_\u001b[0;34m(parent, func, args, kwargs)\u001b[0m\n\u001b[1;32m   2780\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2781\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m strict_ctx:\n\u001b[0;32m-> 2782\u001b[0m         \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2783\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObservedException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   2784\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObserved exception DURING INLING \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:893\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    892\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mpush_tx(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 893\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    894\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    895\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:805\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_block_stack(inst)\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 805\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43minst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopcode\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mshould_exit\n\u001b[1;32m    807\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObservedException:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:234\u001b[0m, in \u001b[0;36mstack_op.<locals>.impl\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimpl\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstructionTranslatorBase\u001b[39m\u001b[38;5;124m\"\u001b[39m, inst: Instruction):\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpush(\u001b[43mfn_var\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpopn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/variables/builtin.py:962\u001b[0m, in \u001b[0;36mBuiltinVariable.call_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m handler:\n\u001b[1;32m    959\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_function_handler_cache[key] \u001b[38;5;241m=\u001b[39m handler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_handler(\n\u001b[1;32m    960\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn, [\u001b[38;5;28mtype\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m args], \u001b[38;5;28mbool\u001b[39m(kwargs)\n\u001b[1;32m    961\u001b[0m     )\n\u001b[0;32m--> 962\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/variables/builtin.py:941\u001b[0m, in \u001b[0;36mBuiltinVariable._handle_insert_op_in_graph\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    937\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m fn \u001b[38;5;129;01mis\u001b[39;00m operator\u001b[38;5;241m.\u001b[39mtruediv \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    938\u001b[0m             args[\u001b[38;5;241m0\u001b[39m], variables\u001b[38;5;241m.\u001b[39mUnspecializedPythonVariable\n\u001b[1;32m    939\u001b[0m         ):\n\u001b[1;32m    940\u001b[0m             args[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mconvert_to_constant(tx)\n\u001b[0;32m--> 941\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrap_fx_proxy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[1;32m    944\u001b[0m     unimplemented(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpartial tensor op: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/variables/builder.py:1713\u001b[0m, in \u001b[0;36mwrap_fx_proxy\u001b[0;34m(tx, proxy, example_value, subclass_type, **options)\u001b[0m\n\u001b[1;32m   1705\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1706\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtx\u001b[39m\u001b[38;5;124m\"\u001b[39m: tx,\n\u001b[1;32m   1707\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproxy\u001b[39m\u001b[38;5;124m\"\u001b[39m: proxy,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1710\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions,\n\u001b[1;32m   1711\u001b[0m }\n\u001b[1;32m   1712\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m subclass_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1713\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrap_fx_proxy_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTensorVariable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1715\u001b[0m     result \u001b[38;5;241m=\u001b[39m wrap_fx_proxy_cls(target_cls\u001b[38;5;241m=\u001b[39mTensorWithTFOverrideVariable, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/variables/builder.py:1798\u001b[0m, in \u001b[0;36mwrap_fx_proxy_cls\u001b[0;34m(target_cls, tx, proxy, example_value, subclass_type, **options)\u001b[0m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;66;03m# with preserve_rng_state():\u001b[39;00m\n\u001b[1;32m   1795\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m example_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1796\u001b[0m     \u001b[38;5;66;03m# only allow_non_graph_fake in this instance because we handle the non-fake\u001b[39;00m\n\u001b[1;32m   1797\u001b[0m     \u001b[38;5;66;03m# cases properly below.\u001b[39;00m\n\u001b[0;32m-> 1798\u001b[0m     example_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_fake_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproxy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_non_graph_fake\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1800\u001b[0m \u001b[38;5;66;03m# Handle recursive calls here\u001b[39;00m\n\u001b[1;32m   1801\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m maybe_get_fake_mode(example_value) \u001b[38;5;129;01mis\u001b[39;00m tx\u001b[38;5;241m.\u001b[39mfake_mode:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/utils.py:1853\u001b[0m, in \u001b[0;36mget_fake_value\u001b[0;34m(node, tx, allow_non_graph_fake)\u001b[0m\n\u001b[1;32m   1850\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(cause, \u001b[38;5;167;01mTypeError\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margument\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(cause):\n\u001b[1;32m   1851\u001b[0m         unimplemented(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTypeError \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode\u001b[38;5;241m.\u001b[39mtarget\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcause\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1853\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TorchRuntimeError(\u001b[38;5;28mstr\u001b[39m(e))\u001b[38;5;241m.\u001b[39mwith_traceback(e\u001b[38;5;241m.\u001b[39m__traceback__) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m   1855\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_non_graph_fake:\n\u001b[1;32m   1856\u001b[0m     _ \u001b[38;5;241m=\u001b[39m pytree\u001b[38;5;241m.\u001b[39mtree_map_only(\n\u001b[1;32m   1857\u001b[0m         torch\u001b[38;5;241m.\u001b[39mTensor, functools\u001b[38;5;241m.\u001b[39mpartial(ensure_graph_fake, tx\u001b[38;5;241m=\u001b[39mtx), ret_val\n\u001b[1;32m   1858\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/utils.py:1785\u001b[0m, in \u001b[0;36mget_fake_value\u001b[0;34m(node, tx, allow_non_graph_fake)\u001b[0m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1784\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tx\u001b[38;5;241m.\u001b[39mfake_mode, enable_python_dispatcher():\n\u001b[0;32m-> 1785\u001b[0m         ret_val \u001b[38;5;241m=\u001b[39m \u001b[43mwrap_fake_exception\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1786\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnnmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1787\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported:\n\u001b[1;32m   1789\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/utils.py:1300\u001b[0m, in \u001b[0;36mwrap_fake_exception\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m   1298\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap_fake_exception\u001b[39m(fn):\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1300\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1301\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m UnsupportedFakeTensorException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1302\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m unimplemented\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/utils.py:1786\u001b[0m, in \u001b[0;36mget_fake_value.<locals>.<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1784\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tx\u001b[38;5;241m.\u001b[39mfake_mode, enable_python_dispatcher():\n\u001b[1;32m   1785\u001b[0m         ret_val \u001b[38;5;241m=\u001b[39m wrap_fake_exception(\n\u001b[0;32m-> 1786\u001b[0m             \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[43mrun_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnnmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1787\u001b[0m         )\n\u001b[1;32m   1788\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported:\n\u001b[1;32m   1789\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/utils.py:1921\u001b[0m, in \u001b[0;36mrun_node\u001b[0;34m(tracer, node, args, kwargs, nnmodule)\u001b[0m\n\u001b[1;32m   1919\u001b[0m         unimplemented(make_error_message(e), from_exc\u001b[38;5;241m=\u001b[39me)\n\u001b[1;32m   1920\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1921\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(make_error_message(e))\u001b[38;5;241m.\u001b[39mwith_traceback(\n\u001b[1;32m   1922\u001b[0m             e\u001b[38;5;241m.\u001b[39m__traceback__\n\u001b[1;32m   1923\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(op)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/utils.py:1903\u001b[0m, in \u001b[0;36mrun_node\u001b[0;34m(tracer, node, args, kwargs, nnmodule)\u001b[0m\n\u001b[1;32m   1901\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1902\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m op \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcall_function\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1903\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1904\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m op \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcall_method\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1905\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], node\u001b[38;5;241m.\u001b[39mtarget)(\u001b[38;5;241m*\u001b[39margs[\u001b[38;5;241m1\u001b[39m:], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_stats.py:21\u001b[0m, in \u001b[0;36mcount.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m     simple_call_counter[fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     20\u001b[0m simple_call_counter[fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m] \u001b[38;5;241m=\u001b[39m simple_call_counter[fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py:1061\u001b[0m, in \u001b[0;36mFakeTensorMode.__torch_dispatch__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m   1058\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_dispatch_mode(torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_TorchDispatchModeKey\u001b[38;5;241m.\u001b[39mFAKE) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1059\u001b[0m ), func\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1061\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   1063\u001b[0m     log\u001b[38;5;241m.\u001b[39mexception(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfake tensor raised TypeError\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py:1450\u001b[0m, in \u001b[0;36mFakeTensorMode.dispatch\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1447\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1449\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_enabled:\n\u001b[0;32m-> 1450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cached_dispatch_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1451\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1452\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch_impl(func, types, args, kwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py:1153\u001b[0m, in \u001b[0;36mFakeTensorMode._cached_dispatch_impl\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     FakeTensorMode\u001b[38;5;241m.\u001b[39mcache_bypasses[e\u001b[38;5;241m.\u001b[39mreason] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output \u001b[38;5;129;01mis\u001b[39;00m _UNASSIGNED:\n\u001b[0;32m-> 1153\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py:1671\u001b[0m, in \u001b[0;36mFakeTensorMode._dispatch_impl\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1669\u001b[0m     fast_impl \u001b[38;5;241m=\u001b[39m get_fast_op_impls()\u001b[38;5;241m.\u001b[39mget(func)\n\u001b[1;32m   1670\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m fast_impl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1671\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m maybe_propagate_real_tensors(\u001b[43mfast_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1673\u001b[0m \u001b[38;5;66;03m# If there's a Python meta, prefer that over the decomposition\u001b[39;00m\n\u001b[1;32m   1674\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_decomp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m meta_table \u001b[38;5;28;01mas\u001b[39;00m meta_table\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_subclasses/fake_impls.py:1062\u001b[0m, in \u001b[0;36mmake_fast_binary_impl.<locals>.fast_binary_impl\u001b[0;34m(mode, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1058\u001b[0m         final_shape \u001b[38;5;241m=\u001b[39m shape\n\u001b[1;32m   1059\u001b[0m     \u001b[38;5;66;03m# TODO: Minor optimization: track if the shapes\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m     \u001b[38;5;66;03m# were equal so you can skip the equality check\u001b[39;00m\n\u001b[1;32m   1061\u001b[0m     \u001b[38;5;66;03m# below if unnecessary\u001b[39;00m\n\u001b[0;32m-> 1062\u001b[0m     final_shape \u001b[38;5;241m=\u001b[39m \u001b[43minfer_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m final_shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1065\u001b[0m \u001b[38;5;66;03m# Do some extra safety checks to see if the output\u001b[39;00m\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;66;03m# stride is obvious\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_subclasses/fake_impls.py:1016\u001b[0m, in \u001b[0;36minfer_size\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m   1003\u001b[0m     sizeB \u001b[38;5;241m=\u001b[39m b[dimB] \u001b[38;5;28;01mif\u001b[39;00m dimB \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1005\u001b[0m     \u001b[38;5;66;03m# NB: It is very important to test for broadcasting, before testing\u001b[39;00m\n\u001b[1;32m   1006\u001b[0m     \u001b[38;5;66;03m# sizeA == sizeB.  This is because the broadcasting tests are likely\u001b[39;00m\n\u001b[1;32m   1007\u001b[0m     \u001b[38;5;66;03m# to be statically known (in particular, if sizeA/sizeB is unbacked\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1014\u001b[0m     \u001b[38;5;66;03m# were not the case, we'd need to write this using torch.sym_or() or\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m     \u001b[38;5;66;03m# something like that).\u001b[39;00m\n\u001b[0;32m-> 1016\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mguard_size_oblivious\u001b[49m\u001b[43m(\u001b[49m\u001b[43msizeA\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mguard_size_oblivious\u001b[49m\u001b[43m(\u001b[49m\u001b[43msizeB\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msizeA\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msizeB\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mThe size of tensor a (\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msizeA\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m) \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmust match the size of tensor b (\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msizeB\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m) \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mat non-singleton dimension \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1024\u001b[0m     expandedSizes[i] \u001b[38;5;241m=\u001b[39m sizeB \u001b[38;5;28;01mif\u001b[39;00m guard_size_oblivious(sizeA \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m sizeA\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(expandedSizes)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/__init__.py:1353\u001b[0m, in \u001b[0;36m_check\u001b[0;34m(cond, message)\u001b[0m\n\u001b[1;32m   1338\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check\u001b[39m(cond, message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[1;32m   1339\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Throws error containing an optional message if the specified condition\u001b[39;00m\n\u001b[1;32m   1340\u001b[0m \u001b[38;5;124;03m    is False.\u001b[39;00m\n\u001b[1;32m   1341\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1351\u001b[0m \u001b[38;5;124;03m            message. Default: ``None``\u001b[39;00m\n\u001b[1;32m   1352\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1353\u001b[0m     \u001b[43m_check_with\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;167;43;01mRuntimeError\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/__init__.py:1336\u001b[0m, in \u001b[0;36m_check_with\u001b[0;34m(error_type, cond, message)\u001b[0m\n\u001b[1;32m   1332\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage must be a callable\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1334\u001b[0m     message_evaluated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(message())\n\u001b[0;32m-> 1336\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m error_type(message_evaluated)\n",
      "\u001b[0;31mTorchRuntimeError\u001b[0m: Failed running call_function <built-in function add>(*(FakeTensor(..., device='cuda:0', size=(32, s1, 64),\n           grad_fn=<EmbeddingBackward0>), FakeTensor(..., device='cuda:0', size=(1, 512, 64))), **{}):\nThe size of tensor a (s1) must match the size of tensor b (512) at non-singleton dimension 1)\n\nfrom user code:\n   File \"/tmp/ipykernel_1096/1340379155.py\", line 8, in forward\n    enc_output = self.encoder(enc_input, enc_mask)\n  File \"/home/wei/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/ipykernel_1096/300891069.py\", line 28, in forward\n    x = self.pos_encoding(x)\n  File \"/home/wei/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/ipykernel_1096/1597975147.py\", line 16, in forward\n    x = x + self.pe[:, :x.size(1), :]\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
     ]
    }
   ],
   "source": [
    "mode = [ 'max-autotune']\n",
    "epoch_times = []\n",
    "for m in mode:\n",
    "    # print(f\"\\n{op =}\" )\n",
    "    #reset model\n",
    "    encoder = Encoder(vocab_size=tokenizer.vocab_size + 1,\n",
    "                  max_len=512,\n",
    "                  d_k=16,\n",
    "                  d_model=64,\n",
    "                  n_heads=4,\n",
    "                  n_layers=2,\n",
    "                  dropout_prob=0.1)\n",
    "    decoder = Decoder(vocab_size=tokenizer.vocab_size + 1,\n",
    "                      max_len=512,\n",
    "                      d_k=16,\n",
    "                      d_model=64,\n",
    "                      n_heads=4,\n",
    "                      n_layers=2,\n",
    "                      dropout_prob=0.1)\n",
    "    transformer = Transformer(encoder,decoder)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "    # encoder.to(device)\n",
    "    # decoder.to(device)\n",
    "    if m in [\"default\",\"reduce-overhead\",'max-autotune']:\n",
    "        # encoder = torch.compile(encoder,mode=m,backend='inductor')\n",
    "        # decoder = torch.compile(decoder,mode=m,backend='inductor')\n",
    "        # transformer = torch.compile(transformer,mode=m,backend='inductor')\n",
    "        transformer = torch.compile(transformer, mode=m, backend='inductor')\n",
    "\n",
    "        print(\"Torch compile training---\")\n",
    "    # encoder.to(device)\n",
    "    # decoder.to(device)\n",
    "    transformer.to(device)\n",
    "\n",
    "    transformer= torch.nn.DataParallel(transformer)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    \n",
    "\n",
    "    train_losses, test_losses,time_epoch,time_dataloading,time_training,train_accuracy,test_accuracy = train(\n",
    "        transformer, criterion, optimizer, train_loader, valid_loader, epochs=5)\n",
    "\n",
    "    epoch_times.append(np.mean(time_epoch))\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47c8130-b976-4057-b15e-93534a76a690",
   "metadata": {},
   "source": [
    "# Optimizing Inference Speed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a68e50b-e105-4cf2-890a-18e965cdae69",
   "metadata": {},
   "source": [
    "## Flash Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04535397-fc8e-426a-83ca-0e0f331168db",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export LD_LIBRARY_PATH=/usr/local/cuda-12/lib64:$LD_LIBRARY_PATH\n",
    "!export PATH=$PATH:/usr/local/cuda-12/bin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "671e4326-c90f-4798-9382-88dc6b4fb061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2024 NVIDIA Corporation\n",
      "Built on Thu_Sep_12_02:18:05_PDT_2024\n",
      "Cuda compilation tools, release 12.6, V12.6.77\n",
      "Build cuda_12.6.r12.6/compiler.34841621_0\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting flash-attn\n",
      "  Using cached flash_attn-2.7.0.post2.tar.gz (2.7 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting einops\n",
      "  Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 KB\u001b[0m \u001b[31m772.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mMB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch in /home/wei/.local/lib/python3.10/site-packages (from flash-attn) (2.4.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/wei/.local/lib/python3.10/site-packages (from torch->flash-attn) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/wei/.local/lib/python3.10/site-packages (from torch->flash-attn) (2.20.5)\n",
      "Requirement already satisfied: filelock in /home/wei/.local/lib/python3.10/site-packages (from torch->flash-attn) (3.16.1)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/wei/.local/lib/python3.10/site-packages (from torch->flash-attn) (12.1.105)\n",
      "Requirement already satisfied: sympy in /home/wei/.local/lib/python3.10/site-packages (from torch->flash-attn) (1.13.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/wei/.local/lib/python3.10/site-packages (from torch->flash-attn) (4.12.2)\n",
      "Requirement already satisfied: triton==3.0.0 in /home/wei/.local/lib/python3.10/site-packages (from torch->flash-attn) (3.0.0)\n",
      "Requirement already satisfied: jinja2 in /home/wei/.local/lib/python3.10/site-packages (from torch->flash-attn) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/wei/.local/lib/python3.10/site-packages (from torch->flash-attn) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/wei/.local/lib/python3.10/site-packages (from torch->flash-attn) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/wei/.local/lib/python3.10/site-packages (from torch->flash-attn) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/wei/.local/lib/python3.10/site-packages (from torch->flash-attn) (10.3.2.106)\n",
      "Requirement already satisfied: fsspec in /home/wei/.local/lib/python3.10/site-packages (from torch->flash-attn) (2024.9.0)\n",
      "Requirement already satisfied: networkx in /home/wei/.local/lib/python3.10/site-packages (from torch->flash-attn) (3.4)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/wei/.local/lib/python3.10/site-packages (from torch->flash-attn) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/wei/.local/lib/python3.10/site-packages (from torch->flash-attn) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/wei/.local/lib/python3.10/site-packages (from torch->flash-attn) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/wei/.local/lib/python3.10/site-packages (from torch->flash-attn) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/wei/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->flash-attn) (12.6.77)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/wei/.local/lib/python3.10/site-packages (from jinja2->torch->flash-attn) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/wei/.local/lib/python3.10/site-packages (from sympy->torch->flash-attn) (1.3.0)\n",
      "Building wheels for collected packages: flash-attn\n",
      "  Building wheel for flash-attn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for flash-attn: filename=flash_attn-2.7.0.post2-cp310-cp310-linux_x86_64.whl size=183278540 sha256=eb4eb73682931a7c591a97ff12926b392ebea4c67e8d9846537925fbfcddaecf\n",
      "  Stored in directory: /home/wei/.cache/pip/wheels/bf/e3/ed/5e845387d52f2debd1bafb847bf3d774d3f0a3c8e31b1dc948\n",
      "Successfully built flash-attn\n",
      "Installing collected packages: einops, flash-attn\n",
      "Successfully installed einops-0.8.0 flash-attn-2.7.0.post2\n"
     ]
    }
   ],
   "source": [
    "!export CUDA_HOME=/usr/local/cuda-12 && export LD_LIBRARY_PATH=/usr/local/cuda-12/lib64:$LD_LIBRARY_PATH && export PATH=/usr/local/cuda-12/bin:$PATH && nvcc --version && pip install flash-attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79ea7145-4fbd-42a4-879e-5d41b09cfed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting nvidia-pyindex\n",
      "  Downloading nvidia-pyindex-1.0.9.tar.gz (10 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: nvidia-pyindex\n",
      "  Building wheel for nvidia-pyindex (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nvidia-pyindex: filename=nvidia_pyindex-1.0.9-py3-none-any.whl size=8416 sha256=80b2ed94eaa6f8f9fe09041a7d0bbb6cb5dd9f257abe34e5fd8233a7f8d2dfcf\n",
      "  Stored in directory: /home/wei/.cache/pip/wheels/2c/af/d0/7a12f82cab69f65d51107f48bcd6179e29b9a69a90546332b3\n",
      "Successfully built nvidia-pyindex\n",
      "Installing collected packages: nvidia-pyindex\n",
      "Successfully installed nvidia-pyindex-1.0.9\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "\u001b[33mDEPRECATION: The HTML index page being used (https://developer.download.nvidia.com/compute/redist/nvidia-cuda-nvcc/) is not a proper HTML 5 document. This is in violation of PEP 503 which requires these pages to be well-formed HTML 5 documents. Please reach out to the owners of this index page, and ask them to update this index page to a valid HTML 5 document. pip 22.2 will enforce this behaviour change. Discussion can be found at https://github.com/pypa/pip/issues/10825\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting nvidia-cuda-nvcc\n",
      "  Downloading https://developer.download.nvidia.com/compute/redist/nvidia-cuda-nvcc/nvidia_cuda_nvcc-11.3.58-py3-none-manylinux1_x86_64.whl (16.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from nvidia-cuda-nvcc) (59.6.0)\n",
      "Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from nvidia-cuda-nvcc) (0.37.1)\n",
      "Installing collected packages: nvidia-cuda-nvcc\n",
      "Successfully installed nvidia-cuda-nvcc-11.3.58\n"
     ]
    }
   ],
   "source": [
    "!pip install nvidia-pyindex\n",
    "!pip install nvidia-cuda-nvcc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd00caf8-f567-4842-a9a1-784812d08fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2021 NVIDIA Corporation\n",
      "Built on Thu_Nov_18_09:45:30_PST_2021\n",
      "Cuda compilation tools, release 11.5, V11.5.119\n",
      "Build cuda_11.5.r11.5/compiler.30672275_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c83a96b-ce80-48c2-bc3f-4d1070cb1d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install flash-attn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd695a37-12ae-487d-bad3-26f79397291f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pytorch --version\n",
    "import flash_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8051e89-207c-4ac1-90e2-8c1776b9a9fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'flash_attn_unpadded_func' from 'flash_attn.flash_attn_interface' (/home/wei/.local/lib/python3.10/site-packages/flash_attn/flash_attn_interface.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mTransformer_flash_attention\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Encoder,Decoder,Transformer_flash_attention\n",
      "File \u001b[0;32m~/github/HPML/Project/models/Transformer_flash_attention.py:6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mflash_attn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mflash_attn_interface\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m flash_attn_unpadded_func  \u001b[38;5;66;03m# Import Flash Attention\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mMultiHeadAttention\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, d_k, d_model, n_heads, max_len, causal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'flash_attn_unpadded_func' from 'flash_attn.flash_attn_interface' (/home/wei/.local/lib/python3.10/site-packages/flash_attn/flash_attn_interface.py)"
     ]
    }
   ],
   "source": [
    "from models.Transformer_flash_attention import Encoder,Decoder,Transformer_flash_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e008fa8-f2df-4396-b3b4-4996410d03a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43ffec9-d19a-4492-87a1-a34b8f3588ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3ec71d-b1e0-4cfb-a1f9-669aa105b757",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855dcadd-9b21-4d63-baed-0a161e7466bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3273b9-2bd8-452b-a9ec-a0027edf7b76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b569cc50-73df-4f3d-96ff-062e3ff4f2b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575bc031-9020-40ed-b2f3-1012fa324c03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5295c75-b1d4-409b-bae1-3a3b66df456c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e182423d-6f3e-4aa6-ba66-7a4a58e40428",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495f1563-9ee0-4f4b-9070-73abcf464e6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a0fa3d-ea89-4045-89c7-3fa7fcf0be4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f5d478-107f-4636-bd77-2854bf96133b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080f951e-9232-4611-a3b5-cacd6b7a17c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31750d25-cf2d-4750-970a-9d9ee8e6a14c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a72d14e-1cff-4856-a5e6-4ab568cd2c02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c9f392-197d-4600-b08c-1bc1095c2383",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d76d7cd-149d-4316-9280-22246473ca99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9d4e87-812f-409b-8c4c-31b182df2c2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86d7635-67db-442b-9bcd-78bcf3b022f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9391d7b4-7577-4e89-807c-8ee0fb72205a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "698c17cc-683a-4fb3-b8c0-3f9bc68e7baa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id</td>\n",
       "      <td>article</td>\n",
       "      <td>highlights</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0001d1afc246a7964130f43ae940af6bc6c57f01</td>\n",
       "      <td>By . Associated Press . PUBLISHED: . 14:11 EST...</td>\n",
       "      <td>Bishop John Folda, of North Dakota, is taking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0002095e55fcbd3a2f366d9bf92a95433dc305ef</td>\n",
       "      <td>(CNN) -- Ralph Mata was an internal affairs li...</td>\n",
       "      <td>Criminal complaint: Cop used his role to help ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00027e965c8264c35cc1bc55556db388da82b07f</td>\n",
       "      <td>A drunk driver who killed a young woman in a h...</td>\n",
       "      <td>Craig Eccleston-Todd, 27, had drunk at least t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0002c17436637c4fe1837c935c04de47adb18e9a</td>\n",
       "      <td>(CNN) -- With a breezy sweep of his pen Presid...</td>\n",
       "      <td>Nina dos Santos says Europe must be ready to a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          0  \\\n",
       "0                                        id   \n",
       "1  0001d1afc246a7964130f43ae940af6bc6c57f01   \n",
       "2  0002095e55fcbd3a2f366d9bf92a95433dc305ef   \n",
       "3  00027e965c8264c35cc1bc55556db388da82b07f   \n",
       "4  0002c17436637c4fe1837c935c04de47adb18e9a   \n",
       "\n",
       "                                                   1  \\\n",
       "0                                            article   \n",
       "1  By . Associated Press . PUBLISHED: . 14:11 EST...   \n",
       "2  (CNN) -- Ralph Mata was an internal affairs li...   \n",
       "3  A drunk driver who killed a young woman in a h...   \n",
       "4  (CNN) -- With a breezy sweep of his pen Presid...   \n",
       "\n",
       "                                                   2  \n",
       "0                                         highlights  \n",
       "1  Bishop John Folda, of North Dakota, is taking ...  \n",
       "2  Criminal complaint: Cop used his role to help ...  \n",
       "3  Craig Eccleston-Todd, 27, had drunk at least t...  \n",
       "4  Nina dos Santos says Europe must be ready to a...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !head data/cnn_dailymail/train.csv\n",
    "import pandas as pd\n",
    "df = pd.read_csv('data/cnn_dailymail/train.csv', sep=\",\", header=None)\n",
    "df_test = pd.read_csv('data/cnn_dailymail/test.csv', sep=\",\", header=None)\n",
    "df = df.iloc[:10000]\n",
    "df_test = df_test.iloc[:1000]\n",
    "\n",
    "df.to_csv('data/cnn_dailymail/train_short.csv', index=None)\n",
    "df_test.to_csv('data/cnn_dailymail/test_short.csv', index=None)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a0e094b8-8022-455c-a70a-7b95929d1a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c34959f2-4618-4036-9ac6-133dfe663d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3521 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max token in col1: 3521\n",
      "Max token in col2: 926\n"
     ]
    }
   ],
   "source": [
    "maxinput_len = df[1].str.len().max()\n",
    "maxoutput_len = df[2].str.len().max()\n",
    "maxinput_tokens = df[df[1].str.len() == maxinput_len][1].apply(lambda x: len(tokenizer.tokenize(x)))\n",
    "maxoutput_tokens = df[df[2].str.len() == maxoutput_len][2].apply(lambda x: len(tokenizer.tokenize(x)))\n",
    "maxinput_tokens=maxinput_tokens.max()\n",
    "maxoutput_tokens = maxoutput_tokens.max()\n",
    "print(\"Max token in col1:\", maxinput_tokens)\n",
    "print(\"Max token in col2:\", maxoutput_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0ebb67f3-8723-45ad-b16a-122b1ab9c490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maxinput_tokens = df[1].apply(lambda x: len(tokenizer.tokenize(x)))\n",
    "# maxoutput_tokens = df[2].apply(lambda x: len(tokenizer.tokenize(x)))\n",
    "# maxinput_tokens=maxinput_tokens.max()\n",
    "# maxoutput_tokens=maxoutput_tokens.max()\n",
    "# print(f\"{maxinput_tokens=}\")\n",
    "# print(f\"{maxoutput_tokens=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a5fba5be-0a5e-4cf7-a751-4828c43f09dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # removing the id fiels\n",
    "# df.column =[\"article\", \"highlights\"]\n",
    "# df.to_csv(\"/data/cnn_dailymail\"\n",
    "# print(f\"{df.shape = }\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6d2520da-ea92-4c89-a4e3-70444aad26d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/wei/.local/lib/python3.10/site-packages (4.46.2)\n",
      "Requirement already satisfied: datasets in /home/wei/.local/lib/python3.10/site-packages (3.1.0)\n",
      "Requirement already satisfied: sentencepiece in /home/wei/.local/lib/python3.10/site-packages (0.2.0)\n",
      "Requirement already satisfied: sacremoses in /home/wei/.local/lib/python3.10/site-packages (0.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/wei/.local/lib/python3.10/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/wei/.local/lib/python3.10/site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: filelock in /home/wei/.local/lib/python3.10/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: requests in /home/wei/.local/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/wei/.local/lib/python3.10/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/wei/.local/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/wei/.local/lib/python3.10/site-packages (from transformers) (4.67.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/wei/.local/lib/python3.10/site-packages (from transformers) (2.1.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/wei/.local/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/wei/.local/lib/python3.10/site-packages (from datasets) (18.0.0)\n",
      "Requirement already satisfied: xxhash in /home/wei/.local/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: pandas in /home/wei/.local/lib/python3.10/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: aiohttp in /home/wei/.local/lib/python3.10/site-packages (from datasets) (3.11.2)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/wei/.local/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/wei/.local/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec[http]<=2024.9.0,>=2023.1.0 in /home/wei/.local/lib/python3.10/site-packages (from datasets) (2024.9.0)\n",
      "Requirement already satisfied: joblib in /home/wei/.local/lib/python3.10/site-packages (from sacremoses) (1.4.2)\n",
      "Requirement already satisfied: click in /home/wei/.local/lib/python3.10/site-packages (from sacremoses) (8.1.7)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/wei/.local/lib/python3.10/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/wei/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.17.1)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/wei/.local/lib/python3.10/site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/wei/.local/lib/python3.10/site-packages (from aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/wei/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/wei/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/wei/.local/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/wei/.local/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/wei/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/wei/.local/lib/python3.10/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/wei/.local/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/wei/.local/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/wei/.local/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/wei/.local/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/wei/.local/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/wei/.local/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets sentencepiece sacremoses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b8733686-d950-4925-ba47-6afaad861e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\n",
    "maxinput_tokens = 3521\n",
    "maxoutput_tokens = 926\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8b79f28c-9db9-4d4c-a215-bbc120724ade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abc0f41f98e442d8b2ff9992eacdeb31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dab98b821b1b4d44a49ba005999f86c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "train_dataset = load_dataset('csv', data_files='data/cnn_dailymail/train_short.csv')\n",
    "test_dataset = load_dataset('csv', data_files='data/cnn_dailymail/test_short.csv')\n",
    "train_dataset = train_dataset.remove_columns(\"0\")\n",
    "test_dataset = test_dataset.remove_columns(\"0\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f096844a-ca50-4a43-8254-e2a50b9b90c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['1', '2'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b0c1371d-3864-4b22-b4c9-5a7f6db1fea1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['1', '2'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "114b01db-950f-4f5d-9f10-1e7cc8b7ab11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "# checkpoint = 'distilbert-base-cased'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "96134fc1-863c-436f-835b-416db8453cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"By . Associated Press . PUBLISHED: . 14:11 EST, 25 October 2013 . | . UPDATED: . 15:36 EST, 25 October 2013 . The bishop of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo, Grand Forks and Jamestown to the hepatitis A virus in late September and early October. The state Health Department has issued an advisory of exposure for anyone who attended five churches and took communion. Bishop John Folda (pictured) of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo, Grand Forks and Jamestown to the hepatitis A . State Immunization Program Manager Molly Howell says the risk is low, but officials feel it's important to alert people to the possible exposure. The diocese announced on Monday that Bishop John Folda is taking time off after being diagnosed with hepatitis A. The diocese says he contracted the infection through contaminated food while attending a conference for newly ordained bishops in Italy last month. Symptoms of hepatitis A include fever, tiredness, loss of appetite, nausea and abdominal discomfort. Fargo Catholic Diocese in North Dakota (pictured) is where the bishop is located .\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article1 = train_dataset[\"train\"][1][\"1\"]\n",
    "article1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9f707e9b-076b-4993-8a18-8160165e8b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bishop John Folda, of North Dakota, is taking time off after being diagnosed .\\nHe contracted the infection through contaminated food in Italy .\\nChurch members in Fargo, Grand Forks and Jamestown could have been exposed .'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highlights1 = train_dataset[\"train\"][1][\"2\"]\n",
    "highlights1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0f5370cf-16f3-429e-bc84-6f27a6540250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'Bishop',\n",
       " 'John',\n",
       " 'F',\n",
       " '##old',\n",
       " '##a',\n",
       " ',',\n",
       " 'of',\n",
       " 'North',\n",
       " 'Dakota',\n",
       " ',',\n",
       " 'is',\n",
       " 'taking',\n",
       " 'time',\n",
       " 'off',\n",
       " 'after',\n",
       " 'being',\n",
       " 'diagnosed',\n",
       " '.',\n",
       " 'He',\n",
       " 'contracted',\n",
       " 'the',\n",
       " 'infection',\n",
       " 'through',\n",
       " 'contaminated',\n",
       " 'food',\n",
       " 'in',\n",
       " 'Italy',\n",
       " '.',\n",
       " 'Church',\n",
       " 'members',\n",
       " 'in',\n",
       " 'Fargo',\n",
       " ',',\n",
       " 'Grand',\n",
       " 'Fork',\n",
       " '##s',\n",
       " 'and',\n",
       " 'James',\n",
       " '##town',\n",
       " 'could',\n",
       " 'have',\n",
       " 'been',\n",
       " 'exposed',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = tokenizer(text_target=highlights1)\n",
    "tokenizer.convert_ids_to_tokens(targets['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2b8a0415-de86-4b05-afa9-d7edc50a83da",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = maxinput_tokens\n",
    "max_target_length = maxoutput_tokens\n",
    "\n",
    "def preprocess_function(batch):\n",
    "    model_inputs = tokenizer(\n",
    "        batch['1'], max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Set up the tokenizer for targets\n",
    "    labels = tokenizer(\n",
    "        text_target=batch['2'], max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ac530761-bb6e-4f37-be14-81e2b9871c55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d7cc7e178e847548ba6a78bf7364ba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = train_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2525b3d4-a00a-432a-860b-358c900647cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "04f8a310-0515-4226-826a-7d5fdba49545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30ed93b607b745a583a4574ac72b2861",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets_test = test_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=test_dataset[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4a864c93-7412-437b-af85-307144e67633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1509e2c2-a86c-41ed-ba7f-bfc2984f8709",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "960ba538-3bc0-4862-90d5-5e7f958c66bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'labels'])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(0, 5)])\n",
    "batch.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9a22bc4a-776f-4c19-a144-fe448a34cc57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  3342,   102,  ...,     0,     0,     0],\n",
       "        [  101,  1650,   119,  ...,     0,     0,     0],\n",
       "        [  101,   113, 13597,  ...,     0,     0,     0],\n",
       "        [  101,   138,  6882,  ...,  2774,   119,   102],\n",
       "        [  101,   113, 13597,  ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "88906053-232e-4ea4-bc6b-1d5f7428f446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['attention_mask']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d6086204-0e5b-4366-99d0-b1f1ae42d9df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101, 12976,   102,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100],\n",
       "        [  101,  3167,  1287,   143, 11015,  1161,   117,  1104,  1456,  7241,\n",
       "           117,  1110,  1781,  1159,  1228,  1170,  1217, 11534,   119,  1124,\n",
       "         11058,  1103,  8974,  1194, 21636,  2094,  1107,  2413,   119,  1722,\n",
       "          1484,  1107, 25727,   117,  2224, 16384,  1116,  1105,  1600,  3305,\n",
       "          1180,  1138,  1151,  5490,   119,   102,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100],\n",
       "        [  101, 10382, 12522,   131,  3291,  1643,  1215,  1117,  1648,  1106,\n",
       "          1494, 18316,  3404,  8811,   119,  6197, 28044,   117,  1126,  4422,\n",
       "          5707,  6403,   117,  9273,  2375,  1372,  1243,  3832,   119,  1124,\n",
       "          1145,  4768,  1106,  2653,  1160, 27459,  1107,   170,  3513,  4928,\n",
       "           117,   170, 12522,  1155, 27487,  1116,   119,   102,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100],\n",
       "        [  101,  6422,   142, 19515,  2897,  1633,   118,  6365,   117,  1765,\n",
       "           117,  1125,  6882,  1120,  1655,  1210, 10473,  2145,  1196,  3759,\n",
       "          1610,   119,  3982,  1606,  2179,  1165,  1119,  1396,  5686,  1506,\n",
       "          1812,  1107, 14680,  9019, 16397,   117,  8843,  1104, 23721,   119,\n",
       "         21229,  1174,  1246,   118,  1113,  1154,  1743,   118,  1214,   118,\n",
       "          1385,  4858, 11772,  1183,   112,   188,  1610,   117,  1150,  1452,\n",
       "          1107,  2704,   119,  3284,  1474,  1119,  1156,  1138,  1151,  1166,\n",
       "          2732,  3668,   118,  2797,  5310,  1120,  1159,  1104,  5683,   119,\n",
       "          1124,  1108,  1276,  5425,  1120, 10867,  5373,  2031,  1104,  3989,\n",
       "          1473,  1118,  4249,  3759,   119,   102],\n",
       "        [  101,  9394, 18463, 11064,  1867,  1980,  1538,  1129,  2407,  1106,\n",
       "          4392, 19344,  1209,  2644,  1241,  3091,   119, 18525,  1158,  2733,\n",
       "           112,   188,  1671,  1661,  1156,  1129,  1141,  1236,  1104, 21718,\n",
       "         13044,  1147,  1619,  1111,  1697, 24489,   117,  1131,  1867,   119,\n",
       "          1252,  1131,  1867,  1980,  1156,  1138,   170,  1662,  1159,  3709,\n",
       "          1157, 11615,  1280,  1443,  1540,  1121,  1103,  1746,   119,   102,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['labels']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0724f805-416f-4a3a-b35d-40c36dd178e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100, 102, 0, 101, 103]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.all_special_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a7960138-170a-4903-9d3f-8b9625b812c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.all_special_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b13c58c3-9e4e-4b8e-a64a-3fa205b3077b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=32,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    tokenized_datasets_test[\"train\"],\n",
    "    batch_size=32,\n",
    "    collate_fn=data_collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "222534cc-93b0-4b21-bea2-a68712312db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k: input_ids v.shape: torch.Size([32, 2174])\n",
      "k: attention_mask v.shape: torch.Size([32, 2174])\n",
      "k: labels v.shape: torch.Size([32, 126])\n"
     ]
    }
   ],
   "source": [
    "# check how it works\n",
    "for batch in train_loader:\n",
    "  for k, v in batch.items():\n",
    "    print(\"k:\", k, \"v.shape:\", v.shape)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d4987975-ff8b-4e6b-946f-5e5f1931106e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28996"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "265b9afd-0e79-4918-82a4-dba499ff2271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'championship'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([2899])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9524eb78-c833-434e-ba19-e56ce28fd146",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4eb4fc7c-0c9a-47db-ab75-fdb84290b257",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import dataset\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1a1c8438-598b-44e6-bfca-5decafc0ea07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, d_k, d_model, n_heads, max_len, causal=False):\n",
    "    super().__init__()\n",
    "\n",
    "    # Assume d_v = d_k\n",
    "    self.d_k = d_k\n",
    "    self.n_heads = n_heads\n",
    "\n",
    "    self.key = nn.Linear(d_model, d_k * n_heads)\n",
    "    self.query = nn.Linear(d_model, d_k * n_heads)\n",
    "    self.value = nn.Linear(d_model, d_k * n_heads)\n",
    "\n",
    "    # final linear layer\n",
    "    self.fc = nn.Linear(d_k * n_heads, d_model)\n",
    "\n",
    "    # causal mask\n",
    "    # make it so that diagonal is 0 too\n",
    "    # this way we don't have to shift the inputs to make targets\n",
    "    self.causal = causal\n",
    "    if causal:\n",
    "      cm = torch.tril(torch.ones(max_len, max_len))\n",
    "      self.register_buffer(\n",
    "          \"causal_mask\",\n",
    "          cm.view(1, 1, max_len, max_len)\n",
    "      )\n",
    "\n",
    "  def forward(self, q, k, v, pad_mask=None):\n",
    "    q = self.query(q) # N x T x (hd_k)\n",
    "    k = self.key(k)   # N x T x (hd_k)\n",
    "    v = self.value(v) # N x T x (hd_v)\n",
    "\n",
    "    N = q.shape[0]\n",
    "    T_output = q.shape[1]\n",
    "    T_input = k.shape[1]\n",
    "\n",
    "    # change the shape to:\n",
    "    # (N, T, h, d_k) -> (N, h, T, d_k)\n",
    "    # in order for matrix multiply to work properly\n",
    "    q = q.view(N, T_output, self.n_heads, self.d_k).transpose(1, 2)\n",
    "    k = k.view(N, T_input, self.n_heads, self.d_k).transpose(1, 2)\n",
    "    v = v.view(N, T_input, self.n_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "    # compute attention weights\n",
    "    # (N, h, T, d_k) x (N, h, d_k, T) --> (N, h, T, T)\n",
    "    attn_scores = q @ k.transpose(-2, -1) / math.sqrt(self.d_k)\n",
    "    if pad_mask is not None:\n",
    "      attn_scores = attn_scores.masked_fill(\n",
    "          pad_mask[:, None, None, :] == 0, float('-inf'))\n",
    "    if self.causal:\n",
    "      attn_scores = attn_scores.masked_fill(\n",
    "          self.causal_mask[:, :, :T_output, :T_input] == 0, float('-inf'))\n",
    "    attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "    \n",
    "    # compute attention-weighted values\n",
    "    # (N, h, T, T) x (N, h, T, d_k) --> (N, h, T, d_k)\n",
    "    A = attn_weights @ v\n",
    "\n",
    "    # reshape it back before final linear layer\n",
    "    A = A.transpose(1, 2) # (N, T, h, d_k)\n",
    "    A = A.contiguous().view(N, T_output, self.d_k * self.n_heads) # (N, T, h*d_k)\n",
    "\n",
    "    # projection\n",
    "    return self.fc(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "39582b87-80f5-46e8-97f1-8fc35101d71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "  def __init__(self, d_k, d_model, n_heads, max_len, dropout_prob=0.1):\n",
    "    super().__init__()\n",
    "\n",
    "    self.ln1 = nn.LayerNorm(d_model)\n",
    "    self.ln2 = nn.LayerNorm(d_model)\n",
    "    self.mha = MultiHeadAttention(d_k, d_model, n_heads, max_len, causal=False)\n",
    "    self.ann = nn.Sequential(\n",
    "        nn.Linear(d_model, d_model * 4),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(d_model * 4, d_model),\n",
    "        nn.Dropout(dropout_prob),\n",
    "    )\n",
    "    self.dropout = nn.Dropout(p=dropout_prob)\n",
    "  \n",
    "  def forward(self, x, pad_mask=None):\n",
    "    x = self.ln1(x + self.mha(x, x, x, pad_mask))\n",
    "    x = self.ln2(x + self.ann(x))\n",
    "    x = self.dropout(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5f1d96c7-4264-4a4a-a31b-b590ced48f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "  def __init__(self, d_k, d_model, n_heads, max_len, dropout_prob=0.1):\n",
    "    super().__init__()\n",
    "\n",
    "    self.ln1 = nn.LayerNorm(d_model)\n",
    "    self.ln2 = nn.LayerNorm(d_model)\n",
    "    self.ln3 = nn.LayerNorm(d_model)\n",
    "    self.mha1 = MultiHeadAttention(d_k, d_model, n_heads, max_len, causal=True)\n",
    "    self.mha2 = MultiHeadAttention(d_k, d_model, n_heads, max_len, causal=False)\n",
    "    self.ann = nn.Sequential(\n",
    "        nn.Linear(d_model, d_model * 4),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(d_model * 4, d_model),\n",
    "        nn.Dropout(dropout_prob),\n",
    "    )\n",
    "    self.dropout = nn.Dropout(p=dropout_prob)\n",
    "  \n",
    "  def forward(self, enc_output, dec_input, enc_mask=None, dec_mask=None):\n",
    "    # self-attention on decoder input\n",
    "    x = self.ln1(\n",
    "        dec_input + self.mha1(dec_input, dec_input, dec_input, dec_mask))\n",
    "\n",
    "    # multi-head attention including encoder output\n",
    "    x = self.ln2(x + self.mha2(x, enc_output, enc_output, enc_mask))\n",
    "\n",
    "    x = self.ln3(x + self.ann(x))\n",
    "    x = self.dropout(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a32e10cd-d19f-4e95-b1c2-504a55506bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "  def __init__(self, d_model, max_len=2048, dropout_prob=0.1):\n",
    "    super().__init__()\n",
    "    self.dropout = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "    position = torch.arange(max_len).unsqueeze(1)\n",
    "    exp_term = torch.arange(0, d_model, 2)\n",
    "    div_term = torch.exp(exp_term * (-math.log(10000.0) / d_model))\n",
    "    pe = torch.zeros(1, max_len, d_model)\n",
    "    pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "    pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "    self.register_buffer('pe', pe)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # x.shape: N x T x D\n",
    "    x = x + self.pe[:, :x.size(1), :]\n",
    "    return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7985e3a6-d08e-43b7-854d-3898da0f278a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  def __init__(self,\n",
    "               vocab_size,\n",
    "               max_len,\n",
    "               d_k,\n",
    "               d_model,\n",
    "               n_heads,\n",
    "               n_layers,\n",
    "              #  n_classes,\n",
    "               dropout_prob):\n",
    "    super().__init__()\n",
    "\n",
    "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "    self.pos_encoding = PositionalEncoding(d_model, max_len, dropout_prob)\n",
    "    transformer_blocks = [\n",
    "        EncoderBlock(\n",
    "            d_k,\n",
    "            d_model,\n",
    "            n_heads,\n",
    "            max_len,\n",
    "            dropout_prob) for _ in range(n_layers)]\n",
    "    self.transformer_blocks = nn.Sequential(*transformer_blocks)\n",
    "    self.ln = nn.LayerNorm(d_model)\n",
    "    # self.fc = nn.Linear(d_model, n_classes)\n",
    "  \n",
    "  def forward(self, x, pad_mask=None):\n",
    "    x = self.embedding(x)\n",
    "    x = self.pos_encoding(x)\n",
    "    for block in self.transformer_blocks:\n",
    "      x = block(x, pad_mask)\n",
    "\n",
    "    # many-to-one (x has the shape N x T x D)\n",
    "    # x = x[:, 0, :]\n",
    "\n",
    "    x = self.ln(x)\n",
    "    # x = self.fc(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "251df2eb-cf00-4a6e-a6a9-4eabc2b2bfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "  def __init__(self,\n",
    "               vocab_size,\n",
    "               max_len,\n",
    "               d_k,\n",
    "               d_model,\n",
    "               n_heads,\n",
    "               n_layers,\n",
    "               dropout_prob):\n",
    "    super().__init__()\n",
    "\n",
    "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "    self.pos_encoding = PositionalEncoding(d_model, max_len, dropout_prob)\n",
    "    transformer_blocks = [\n",
    "        DecoderBlock(\n",
    "            d_k,\n",
    "            d_model,\n",
    "            n_heads,\n",
    "            max_len,\n",
    "            dropout_prob) for _ in range(n_layers)]\n",
    "    self.transformer_blocks = nn.Sequential(*transformer_blocks)\n",
    "    self.ln = nn.LayerNorm(d_model)\n",
    "    self.fc = nn.Linear(d_model, vocab_size)\n",
    "  \n",
    "  def forward(self, enc_output, dec_input, enc_mask=None, dec_mask=None):\n",
    "    x = self.embedding(dec_input)\n",
    "    x = self.pos_encoding(x)\n",
    "    for block in self.transformer_blocks:\n",
    "      x = block(enc_output, x, enc_mask, dec_mask)\n",
    "    x = self.ln(x)\n",
    "    x = self.fc(x) # many-to-many\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8827b8b0-385e-44a6-96f5-ac67edb3b7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "  def __init__(self, encoder, decoder):\n",
    "    super().__init__()\n",
    "    self.encoder = encoder\n",
    "    self.decoder = decoder\n",
    "  \n",
    "  def forward(self, enc_input, dec_input, enc_mask, dec_mask):\n",
    "    enc_output = self.encoder(enc_input, enc_mask)\n",
    "    dec_output = self.decoder(enc_output, dec_input, enc_mask, dec_mask)\n",
    "    return dec_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48132b2-b4f0-4ae5-9c22-fbd62c95ff89",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "182cfb8c-9b8c-4076-a1c9-7064322c76cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test it\n",
    "encoder = Encoder(vocab_size=tokenizer.vocab_size,\n",
    "                  max_len=maxinput_tokens,\n",
    "                  d_k=16,\n",
    "                  d_model=64,\n",
    "                  n_heads=4,\n",
    "                  n_layers=2,\n",
    "                  dropout_prob=0.1)\n",
    "decoder = Decoder(vocab_size=tokenizer.vocab_size,\n",
    "                  max_len=maxoutput_tokens,\n",
    "                  d_k=16,\n",
    "                  d_model=64,\n",
    "                  n_heads=4,\n",
    "                  n_layers=2,\n",
    "                  dropout_prob=0.1)\n",
    "transformer = Transformer(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "57ffcadb-b1c1-4a47-a0a5-4e80c2c01a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (embedding): Embedding(28996, 64)\n",
       "  (pos_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_blocks): Sequential(\n",
       "    (0): DecoderBlock(\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha1): MultiHeadAttention(\n",
       "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (mha2): MultiHeadAttention(\n",
       "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (ann): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): DecoderBlock(\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha1): MultiHeadAttention(\n",
       "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (mha2): MultiHeadAttention(\n",
       "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (ann): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  (fc): Linear(in_features=64, out_features=28996, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "encoder.to(device)\n",
    "decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c770c4ea-2019-4873-b443-7481dc4bce97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "optimizer = torch.optim.Adam(transformer.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "884916cb-c5f1-4aa3-b897-83ed18cca24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# A function to encapsulate the training loop\n",
    "def train(model, criterion, optimizer, train_loader, valid_loader, epochs):\n",
    "  train_losses = np.zeros(epochs)\n",
    "  test_losses = np.zeros(epochs)\n",
    "\n",
    "  for it in range(epochs):\n",
    "    model.train()\n",
    "    t0 = datetime.now()\n",
    "    train_loss = []\n",
    "    for batch in train_loader:\n",
    "      # move data to GPU (enc_input, enc_mask, translation)\n",
    "      batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "      # zero the parameter gradients\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      enc_input = batch['input_ids']\n",
    "      enc_mask = batch['attention_mask']\n",
    "      targets = batch['labels']\n",
    "\n",
    "      # shift targets forwards to get decoder_input\n",
    "      dec_input = targets.clone().detach()\n",
    "      dec_input = torch.roll(dec_input, shifts=1, dims=1)\n",
    "      dec_input[:, 0] = 101\n",
    "\n",
    "      # also convert all -100 to pad token id\n",
    "      dec_input = dec_input.masked_fill(\n",
    "          dec_input == -100, tokenizer.pad_token_id)\n",
    "\n",
    "      # make decoder input mask\n",
    "      dec_mask = torch.ones_like(dec_input)\n",
    "      dec_mask = dec_mask.masked_fill(dec_input == tokenizer.pad_token_id, 0)\n",
    "\n",
    "      # Forward pass\n",
    "      outputs = model(enc_input, dec_input, enc_mask, dec_mask)\n",
    "      loss = criterion(outputs.transpose(2, 1), targets)\n",
    "        \n",
    "      # Backward and optimize\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      train_loss.append(loss.item())\n",
    "\n",
    "    # Get train loss and test loss\n",
    "    train_loss = np.mean(train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = []\n",
    "    for batch in valid_loader:\n",
    "      batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "      enc_input = batch['input_ids']\n",
    "      enc_mask = batch['attention_mask']\n",
    "      targets = batch['labels']\n",
    "\n",
    "      # shift targets forwards to get decoder_input\n",
    "      dec_input = targets.clone().detach()\n",
    "      dec_input = torch.roll(dec_input, shifts=1, dims=1)\n",
    "      dec_input[:, 0] = 101\n",
    "\n",
    "      # change -100s to regular padding\n",
    "      dec_input = dec_input.masked_fill(\n",
    "          dec_input == -100, tokenizer.pad_token_id)\n",
    "\n",
    "      # make decoder input mask\n",
    "      dec_mask = torch.ones_like(dec_input)\n",
    "      dec_mask = dec_mask.masked_fill(dec_input == tokenizer.pad_token_id, 0)\n",
    "\n",
    "      outputs = model(enc_input, dec_input, enc_mask, dec_mask)\n",
    "      loss = criterion(outputs.transpose(2, 1), targets)\n",
    "      test_loss.append(loss.item())\n",
    "    test_loss = np.mean(test_loss)\n",
    "\n",
    "    # Save losses\n",
    "    train_losses[it] = train_loss\n",
    "    test_losses[it] = test_loss\n",
    "    \n",
    "    dt = datetime.now() - t0\n",
    "    print(f'Epoch {it+1}/{epochs}, Train Loss: {train_loss:.4f}, \\\n",
    "      Test Loss: {test_loss:.4f}, Duration: {dt}')\n",
    "  \n",
    "  return train_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "221cabac-f732-498a-bf17-d13087d6c2cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.94 GiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. Of the allocated memory 8.66 GiB is allocated by PyTorch, and 3.26 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_losses, test_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[64], line 40\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, criterion, optimizer, train_loader, valid_loader, epochs)\u001b[0m\n\u001b[1;32m     37\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m), targets)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Backward and optimize\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     42\u001b[0m train_loss\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.94 GiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. Of the allocated memory 8.66 GiB is allocated by PyTorch, and 3.26 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "train_losses, test_losses = train(\n",
    "    transformer, criterion, optimizer, train_loader, valid_loader, epochs=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffbc113-9f36-481e-9d38-fadbbd61fd81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85969175-48d7-430e-a5f0-73776efa9e94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263b793b-37c6-4d83-bf37-89a24f8d98df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02b5054-989f-4c70-bdb1-9ddb5e73dec3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097a07b3-a6dc-4f32-aef7-ce486334977b",
   "metadata": {},
   "outputs": [],
   "source": [
    "xe = np.random.randint(0, 20_000, size=(8, 512))\n",
    "xe_t = torch.tensor(xe).to(device)\n",
    "\n",
    "xd = np.random.randint(0, 10_000, size=(8, 256))\n",
    "xd_t = torch.tensor(xd).to(device)\n",
    "\n",
    "maske = np.ones((8, 512))\n",
    "maske[:, 256:] = 0\n",
    "maske_t = torch.tensor(maske).to(device)\n",
    "\n",
    "maskd = np.ones((8, 256))\n",
    "maskd[:, 128:] = 0\n",
    "maskd_t = torch.tensor(maskd).to(device)\n",
    "\n",
    "out = transformer(xe_t, xd_t, maske_t, maskd_t)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e79182-2c08-4ccc-a800-7e4ca56626e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837770ea-1933-4f6c-81de-1aae6ad78b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -nc https://lazyprogrammer.me/course_files/nlp3/spa.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff28450-1990-4a9a-a603-4e3776128fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head spa.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe03718-c635-4b27-8eb3-d3cec668ce0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('spa.txt', sep=\"\\t\", header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5ac2a8-1362-473d-96b7-ba558e2a24a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb10493-2ebb-4e78-a203-3ed892c99251",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[:30_000] # takes too long\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c5105b-87e6-45d6-a380-1e7b0eb14720",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['en', 'es']\n",
    "df.to_csv('spa.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ba7492-4ad0-49fc-847c-b0515d6798a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head spa.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920aa856-03c1-4246-9977-f32747ae1cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets sentencepiece sacremoses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b671983-03a8-4af4-8665-e7c62df2d7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "raw_dataset = load_dataset('csv', data_files='spa.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd99f29-3aff-4a9f-8ab8-2b9a5c15f6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41031af-ca52-4d78-be4e-82149e1e85a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = raw_dataset['train'].train_test_split(test_size=0.3, seed=42)\n",
    "split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cc5c9f-b2bd-4438-982a-ccba62b15b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-es\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d266f6-8183-4624-b009-6b7f5d37388f",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_sentence = split[\"train\"][0][\"en\"]\n",
    "es_sentence = split[\"train\"][0][\"es\"]\n",
    "\n",
    "inputs = tokenizer(en_sentence)\n",
    "targets = tokenizer(text_target=es_sentence)\n",
    "\n",
    "tokenizer.convert_ids_to_tokens(targets['input_ids'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d600aad1-adbd-4d29-8c60-e6bb4190050d",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f52ad9-ce50-467b-8f43-a7a07098f539",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "\n",
    "def preprocess_function(batch):\n",
    "    model_inputs = tokenizer(\n",
    "        batch['en'], max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Set up the tokenizer for targets\n",
    "    labels = tokenizer(\n",
    "        text_target=batch['es'], max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bf838a-f325-4b75-8fd0-b40cdf96eaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = split.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=split[\"train\"].column_names,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c817541-aa1f-4bbd-ab25-918ddc6a9a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb91e842-9df7-45ff-9c90-b055c3716938",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82711a3c-155a-4611-95f8-fe491dd572fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(0, 5)])\n",
    "batch.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bafa28-9fc6-4b3d-926f-a8da60084fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['input_ids']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b827bd02-2175-40a6-89a7-ded746416147",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['attention_mask']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e8c446-330b-44b7-89ac-edaf76f0b894",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['labels']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d25743b-915c-4c47-a9f8-2126d79af7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.all_special_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592d2839-abc4-4e2a-ae06-82c98c5b1eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.all_special_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9990da0c-bb87-4e92-9a1e-69f504d203fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer('<pad>')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95990cba-b9be-4b81-bfab-e207f2bafd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=32,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    tokenized_datasets[\"test\"],\n",
    "    batch_size=32,\n",
    "    collate_fn=data_collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c7c52b-d055-40a3-8bae-a07733fee549",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2625fe5b-069f-49b3-b7d1-294153b83387",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://deeplearningcourses.com/notebooks/YROaDO6P4h3GNPYKYNlWjA/mHntSDrJRP9_eMCl7KMl6w"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
